
\documentclass{sig-alternate}
\usepackage[normalem]{ulem}
\usepackage[usenames,dvipsnames]{pstricks}

\usepackage{mathtools}

\usepackage{algorithm2e}

\usepackage{ctable}
\usepackage{url}
\usepackage{multirow}

\begin{document}
\conferenceinfo{WSDM}{'14 New York City, NY, USA}
\title{Efficient Temporal Synopsis of Social Media Streams}

\maketitle
\begin{abstract}


Search and summarization of streaming social media, such as Twitter, requires the ongoing analysis of large volumes of data with dynamically changing characteristics.  Tweets are short and repetitious~--~lacking context and structure~--~making it difficult to generate a coherent synopsis of events within a given time period.  Although some established algorithms for frequent itemset analysis might provide an efficient foundation for synopsis generation, the unmodified application of standard methods produces a complex mass of rules, dominated by common language constructs and many trivial variations on topically related results.  Moreover, these results are not necessarily specific to events within the time period of interest.  To address these problems, we build upon the Linear time Closed itemset Mining (LCM) algorithm, which is particularly suited to the large and sparse vocabulary of tweets.  LCM generates only closed itemsets, providing an immediate reduction in the number of trivial results.  To reduce the impact of function words and common language constructs, we apply a filtering step that preserves these terms only when they may form part of a relevant collocation.  To further reduce trivial results, we propose a novel strengthening of the closure condition of LCM to retain only those results that exceed a threshold of distinctiveness.  Finally, we perform temporal ranking, based on information gain, to identify results that are particularly relevant to the time period of interest.  We evaluate our work over a collection of tweets gathered in late 2012, exploring the efficiency and filtering characteristic of each processing step, both individually and collectively.  Based on our experience, the resulting synopses from various time periods provide understandable and meaningful pictures of events within those periods, with potential application to tasks such as temporal summarization and query expansion for search.
\end{abstract}
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%\terms{Theory}

%\keywords{ACM proceedings, \LaTeX, text tagging}
\newpage
\section{Introduction}

%The sheer volume of user generated content continuously posted on social media 
%has taken the problem of summarization has taken a totally different dimension 
%gives rise to a new type of problem, or
%User generated content that is continuously posted on social media 
%contains valuable information, which is sometimes not available on other media. 
%The use of social media as a source of information about real world events
%is motivated by the diversity of the content posted on it.


%Users of social media generate a large amount of content 
%about every real world event, internet meme, TV show episode 
%or anything that happens to capture their attention.
%The large number of users making posts about a certain topic
%diversify the content in terms of points of view,
%and their distribution all over the globe increases the propensity
%that some of them will witness an event and make unique posts about it.
%However, the totality of user generated content is overwhelmed 
%by social chatter and non

%The problem of mining social media text is unique 
Mining social media text poses unique challenges
because of the text's content and form.
The user generated content includes a substantial 
amount of social chatter, %and topically irrelevant posts.
which ought to be filtered out by mining algorithms.
In terms of form, social media posts are short thus
undermining the effectiveness of within document 
term frequency counting.
They also lack structure and other formatting cues,
thus important terms cannot be pinpointed easily. 
%Moreover, social media platforms started allowing
%posts to link to each other only recently 
%and in a restricted fashion, 
%thus posts lack context
Moreover, the rate at which users generate and consume
social media posts necessitates the use of efficient algorithms
to provide users by real-time mining results.
To realize the benefits of social media in giving a voice to ordinary people,
it is desirable to devise techniques that focuses on the content of posts.

In this work we study the use of frequent itemset mining
to create a synopsis of social media streams. 
Frequent itemset mining is suitable to the dynamic nature of social media streams.
It does not require prior knowledge of the distribution of items,
nor does it require selecting a few items to monitor 
or a number of topics to mine. %preset
It is fast, efficient and scalable.
%Frequent itemset mining algorithms are fast and efficient,
%however they are not readily suited for application on text.
However, frequent itemset mining is not readily suited for application on text
without preprocessing steps that are difficult to apply to social media text;
for example, stop word removal which requires language identification
to be done before it, and will thus suffer from the poor performance
of language identification on social media~\cite{bergsma2012language}.
In our experience, directly applying frequent itemset mining algorithms
to social media text results in myriads of itemsets that are 
mostly meaningless, repetitive or bear no information.
%Following are the main problems that lead to 
%we address in this work.
In this work we address the shortcomings of 
frequent itemset mining which lead to such poor performance.
%After addressing them, 
As a result, the frequent itemset mining results become
suitable for use as a synopsis representing what is happening
on social media at different points of time. 
%The synopsis is understandable on the user level, 
%and can be used by downstream system components.
%It 
The synopsis is particularly rich because it
includes the itemsets as well as the ids of posts in which they occur.

%First of all, they count the number of occurrences of all 
%combinations of items, 
%which can become prohibit
%The short length of social media posts

%Some frequent itemset robust against data sparsity, 
%and can actually exploit this sparsity for faster computation.


%This family of algorithms is suitable for application 
%on collections of short sets of items,
%regardless of what a set or an item represents.
%It finds items that co-occur together frequently,
%filtering out itemsets with a number of occurrences
%less than a  threshold. %user specified
%which can be set to 

%The nature of text in social media poses a challenge when applying
%traditional text mining algorithms.
%Text in social media is usually short, lacks context and structure,
%and is created at a high rate. %very
%To realize the benefits of social media in giving a voice to ordinary people,
%this work focuses on the content of posts.
%However, the collective stream from all users is overwhelmed with personal
%updates, and timely finding posts about topics of interest requires an
%efficient mining algorithm.

%The first problem is that the number of itemsets mined is large and grows with the number of
%distinct items~--~which is particularly high in the text domain.
Frequent itemset mining was originally proposed as a preliminary stage for
association rules mining.
It produces a large number of itemsets, which
has to be further processed to 
produce a smaller number of association rules.
%which sifts through the numerous itemsets and
%produces a smaller number of association rules.
Furthermore, the number of itemsets grows with the number of
distinct items, which is particularly high in the text domain.
To reduce the number of itemsets, they may be limited by setting a high
frequency threshold, but this is not possible in text mining because
frequencies of items follow a long-tailed Zipfean distribution.
We propose a condition for selecting informative itemsets, 
and a clustering scheme for reducing redundancy in the mining results.
Our methods exploit the dynamics of social media 
and make use of the collaborative filtering
that users naturally undergo on social media, 
by sharing interesting posts and participating in conversations.


%Second, the high frequency of stop words and language constructs is a problem
%that frequent itemset mining is not equipped to handle.
%Even if a maximum frequency threshold is set to filter sets of stop words,
%incurring the risk of filtering important itemsets, 
%many  non-English constructs will be
%mined because the proportion of posts in English is much higher than
%other languages.  Finally, there is considerable redundancy in frequent
%itemsets caused by trivial differences in the language used.
%%In this paper we address those problems, adapting frequent itemset mining to
%%social media text without degrading its efficiency.
%Those are the problems with applying frequent itemset mining to text, 
%following we list some advantages of using it.

%As we shall see in later examples, the top ranked itemsets cover a variety
%of open topics, and within one topic different opinions are reported as
%separate contrastive itemsets.
%The ranking used is according 
%to a formula we propose,
%which orders
%itemsets in terms of their temporal novelty.
%thus turning the synopsis into a summary that 

We process the social media stream in batches 
of data from consecutive epochs of time.
We propose a formula for ranking the itemsets
in the synopsis of each epoch according to temporal novelty.
An empirical evaluation of the effectiveness of our proposed methods
shows that the hour by hour synopses at different times %of different days
actually reflects what was happening on those times. % days.
It is easy to see that
we achieve improvements over a baseline
of applying unmodified frequent itemsets mining, 
in terms of the reduction in the number of itemsets 
and the quality of individual itemsets.
We also make an effort to compare our synopses 
with those generated by a state of the art algorithm 
for summarization based on itemset mining, MTV~\cite{mampaey2011tell}.
%are also shown for comparison. 
However, the number of topically relevant itemsets our algorithm mines 
is much larger than what MTV can mine in reasonable time,
inhibiting the use of standard comparison measures.
The large number of topics in the mining results 
also impede the use of quality measures which require %that need
the creation of a gold standard, 
such as those proposed for evaluating
key-phrase extraction algorithms~\cite{Zhao:2011:TKE:2002472.2002521}.

For the purpose of evaluation, we show the synopses 
of periods of time when it is known what %people should
social media users are expected to be talking about.
Specifically, the day of the U.S. presidential elections,
and periods in which Google Top Charts\footnote{\scriptsize \url{http://www.google.ca/trends/topcharts\#vm=chart&cid=people&geo=US&date=201211}}
show high volume of queries about certain celebrities.
The synopses of all hours in the last 3 months of 2012
is available for download\footnote{\scriptsize \url{http://plg.uwaterloo.ca/~yaboulna/thesis_results/twitter_synopses/1hr+30min_ngram5-relsupp10_oct-nov-dec/}}. 

%The evaluation is only empirical because our work
%results in a synopsis that is made up of itemsets,
%which is different from the summaries generated by
%other extractive or abstractive summarization algorithms.
%The synopsis is most similar to the one generated by 
%key phrase extraction algorithms~\cite{Zhao:2011:TKE:2002472.2002521},
%but the number of topics our algorithm mines is much larger 
%thus inhibiting the construction of a gold standard needed for evaluation
%using the measures used for keyphrase extraction.
%Actually, we find that evaluating our system would best be done
%by an evaluation measure that is suitable for exploratory search tasks.
%However, this is an open problem~\cite{jack} and 
%we leave the creation of an evaluation measure as 
%a potential future direction. 

%we show that the effectiveness of frequent itemsets mining


%Because of using frequent itemset mining, the synopsis includes
As a merit of using frequent itemset mining, the synopses include
itemsets about topics with sustained user interest as well as a spike of interest
--~unlike trending topics\footnote{\scriptsize \url{http://blog.twitter.com/2010/12/to-trend-or-not-to-trend.html}}
\cite{mathioudakis2010twittermonitor} which are limited to spikes of interest.
%Unlike trending
%topics\footnote{\scriptsize http://blog.twitter.com/2010/12/to-trend-or-not-to-trend.html}
%\cite{mathioudakis2010twittermonitor}, the results of frequent itemset
%mining include itemsets that have high frequency because of sustained interest,
%as well as a spike of interest.
The mined itemsets provide the vocabulary associated with events and can be
used as a preliminary step for search and summarization.
For example, the collection of mining results from different epochs of time
can be used for temporal query and document
expansion~\cite{choi2012temporal, efron2012improving}.
The results from each epoch can be treated as a document, facilitating the
creation of a ``temporal profile''~\cite{jones2007temporal} of the query
or a document being expanded.
For summarization, frequent itemsets can provide a good foundation for summary
creation.
While the frequent itemsets themselves are not summaries, since they lack
qualitative properties such as coherence and cohesion, the results are
understandable at the user level.

The rest of the paper is organized as follows.
The next three sections provide necessary background.
We start by discussing related work in section \ref{sec:related},
we then explain frequent itemset mining and the algorithm on which we build
our work in sections \ref{sec:fim} and \ref{sec:lcm} respectively.
In sections \ref{sec:socmine} and \ref{sec:strong} we
propose solutions to the problems faced when applying frequent
itemset mining to social media text. 
Section \ref{sec:socmine} deals with the large number of language
constructs and reduces the number of itemsets.
In section  \ref{sec:strong} we propose a condition for selecting
itemsets and filtering out ones that are not \emph{distinct} from 
their subsets, and we propose a clustering that merges
together similar itemsets into \emph{strongly closed itemsets}.
In section \label{sec:rank} we introduce a ranking formula
specially tailored for ranking \emph{strongly closed itemsets}.
We use this formula to rank itemsets and construct 
the examples used for evaluation. % in section \label{eval}.
Finally, section \ref{sec:concfut} concludes our presentation and
suggests future directions.


\section{Related Work}
\label{sec:related}
Frequent itemset mining comprises a large body of work that goes back to the
early 1990s.
We cover the topic only briefly, as the focus of this paper is not frequent
itemset mining but rather its adaptation to social media text.
The original Apriori algorithm~\cite{agrawal1994fast} and algorithms based on
it suffer performance degradation and  large increases in memory requirement
when the number of distinct items is high.
These limitations are caused by a candidate generation bottleneck,
as explained later.
Another well-known class of mining algorithms are the
FP-Growth~\cite{han2000mining} based algorithms.
FP-Growth skips the candidate generation step, and instead creates a succinct
representation of the data as a frequency ordered prefix tree called the
FP-tree.
An FP-tree imposes the invariant that within each branch the frequency is
non-increasing from the root down to the leaves. 
The memory requirements of the FP-Growth algorithm suffers from the sparsity
of data, since the data structure is succinct only if it can find common
prefixes within the constraints of its invariant. 
Another algorithm, not as widely used but robust against data sparsity, is
Linear-time Closed itemset Mining (LCM)~\cite{uno2004lcm},
which we will describe in detail in section \ref{sec:lcm}.
As a starting point for our work, we use the implementation of LCM submitted
to the workshop for Frequent Itemset Mining Implementations (FIMI) in
2004~\cite{DBLP:conf/fimi/2004}, which was the workshop's award winner. 

The problem of having many itemsets, with redundancy and noise, can be addressed by
picking out  representative itemsets that satisfy a certain condition
that reduces redundancy in the mining results (itemsets and their support information).
The closure condition~\cite{pasquier1999discovering} is a prominent condition
upon which many other conditions are based.
Most similar to the \emph{distinct} itemsets we propose in this paper
are the $\delta$-covered~\cite{xin2005mining} and the
$\delta$-free~\cite{boulicaut2003free} sets.
The $\delta$-covered condition is exactly the opposite of the \emph{distinct}
condition, and it ``relaxes the closure condition to further reduce pattern
set size'' \cite{liu2012finding}.
The $\delta$-free condition is similar to the \emph{distinct} condition,
but it is used to eliminate different itemsets, since its motivation
is to provide a compressed representation of itemsets by sacrificing support
information. 

Another approach to picking out itemsets is choosing ones that 
can be used to compress the data. The KRIMP algorithm~\cite{vreeken2011krimp} is 
a good example of methods that follow this approach. Our goal is different because
we aim to filter out itemsets pertaining to personal updates, which make up a large
portion of social media data. 
A similar goal is sought by the Maximally informaTiVe itemsets (MTV) algorithm~\cite{mampaey2011tell}. 
MTV uses a maximum entropy model to judge if an itemset is redundant, finally choosing the
top K itemsets according to the KL-Diverge between the itemset's  frequency and  the model's estimate. 
In this work we focus on efficiency, allowing the summary to include larger numbers
of itemsets than the values of K for which MTV is efficient.
This is crucial for scalability to the volumes of data typical in social media streams.
However, we compare our results to the ones obtained from MTV.

Likewise, Yan et al.~\cite{yan2005summarizing} choose 
K \emph{master} patterns as representatives of the data. 
A \emph{master} pattern is the union of all itemsets in a 
cluster, similar to our proposed \emph{strongly closed itemset}. 
While the use of clustering is similarly motivated by 
the trivial difference between itemsets, 
the K \emph{master} itemsets  have to cover the whole data, 
unlike the \emph{strongly closed itemsets} which actually 
avoid clustering together different topics or different opinions within a topic.
%Yan:
% A method is proposed for choosing K by minimizing the restoration error of support information, 
%To improve performance, the calculation of the distance between itemsets is approximated to avoid accessing the data. We can achieve high performance without approximation because we apply clustering to subsets of the data that are very likely to be close to each other.

 %Rhetoric:
%The motivation of most research on finding representative itemsets involves
%compressing itemsets mined from a general dataset.
%This motivation leads to decisions different from those required to mine the
%most informative itemsets mined from text.

Our work complements the work done by Yang et al.~\cite{yang2012framework} for using 
frequent itemsets to build a temporal index on social media posts. 
They use frequent itemsets to decrease the space requirements of the index,  
but they choose itemsets to include in the index based on their utility in compressing the data. 
We propose methods to choose itemsets that are topically relevant, 
making use of the nature of social media.
% to choose topically relevant itemsets.
%Our work complements the work done by Yang et al.~\cite{yang2012framework} for using frequent itemsets as a temporal summary. They have proposed a framework for storing mining results of temporally consecutive batches of data using a pyramidal time window. Their framework allows for fast execution of temporal queries, and for tracking the evolution of topics. 
%The choice of itemsets from each batch is based on their utility in compressing the data, and thus they use non-negative matrix factorization to extract topics related to a query. We propose methods to choose itemsets that are topically relevant from each batch, making use of the nature of social media to choose topically relevant itemsets without the need for a query. Social media has been shown to be a good and timely source for discovering real-world events~\cite{ritter2012open,petrovic2010streaming}.

%Our work is also related to methods of indexing social media streams~\cite{yao2012provenance,yang2012framework}. To enhance the speed of indexing, Yao et al.~\cite{yao2012provenance} use... NO they are not related at all!



The use of frequent itemsets for improving search performance has been considered before. 
For example, in Li et al.~\cite{li2010mining} itemsets are mined from
paragraphs of newswire text, and are used to determine term weights for query
expansion.
Improvements in performance have been achieved by using itemsets taken from
a training set of related documents, as well as ones from unrelated documents.
%In Lau et al.~\cite{laumicroblog}, r
Related methods for term weighting were
used in pseudo-relevance feedback for Twitter search~\cite{abounlaga2012frequent,laumicroblog},
and achieved substantial improvements over a baseline.
Our work can provide such methods by a short ranked list of itemsets.
%, potentially improving their results.

%Other efforts for summarizing social media %, specially Twitter
%Many efforts 
%usually take an existing  technique and adapt it for the new medium. 
Existing techniques for summarization and topic extraction were adapted for social media.
Zhao et al.~\cite{Zhao:2011:TKE:2002472.2002521} build summaries based on 
an adaptation of Latent Dirichlet Allocation (LDA) for Twitter.
Parikh and Karlapalem~\cite{parikh2013events} extract topics 
from Tweets using a simplified state machine for burst detection.
The short length of social media text is overcome by 
applying the adapted techniques on large batches of posts made in a long period of time. 
Our method processes data in batches as small as an hour worth of posts, 
resulting in temporally relevant synopses. 
%The number of topics that either of the adaptations discover is small (less than 30) compared to what 
%would be expected to be found in such a large amount of social media posts. 
Petrovi\'{c} et al.~\cite{petrovic2010streaming}
used an adaptation of Locality Sensitive Hashing (LSH) %~\cite{indyk1998approximate} 
to perform first story detection at scale.
Their system clusters tweets efficiently,
but it does not achieve high precision in 
determining if a tweet cluster
pertains to an event or not.

The use of Twitter to provide a summary of a specific event 
has also been studied. 
Chakrabarti and Punera~\cite{chakrabarti2011event} track 
tokens appearing in tweets tagged by the name of an American football team,
and use them to create a summary of a game using statistically improbable phrases.
Sharifi et al.~\cite{sharifi2010summarizing} also use phrases from tweets about a user specified event.
Their method ends up to be very similar to frequent itemset mining,
but uses a graph of terms.

\section{Frequent itemset mining}
\label{sec:fim}
\subsection{Preliminaries}
Classically, frequent itemset mining is applied to a \emph{database} of
\emph{transactions} made at a retail store.
This terminology is suitable for market basket data and we retain it out of
convention, even though we are mining text where the terms ``corpus'' and
``document'' are normally used.
Because of the dynamic nature of social media, rather than giving the whole
database as input to mining algorithms, the input is an \emph{epoch} of data;
data with timestamps within a certain period of time.
The epoch's \emph{span} is the length of this period in hours,
and the \emph{volume velocity} at this epoch is the number of transactions in the epoch
divided by its \emph{span}.

A \emph{frequent itemset} is a set of items that occur together a number of
times higher than a given threshold, called the \emph{support} threshold.
We adapt the support threshold to the dynamicity of the \emph{volume velocity} at
different times of the day.
We define the \emph{minimum support threshold} as the threshold at the hour
of the least \emph{volume velocity} during the day.
The \emph{minimum support} is supplied as an absolute number, $a$, and then
converted to a ratio, $\alpha = \frac{a}{\textbf{avg}(\min_{day}{(vol.\, vel.)})}$.
The actual support threshold used for mining any given epoch is thus
$\alpha$ multiplied by the number of transactions. % in the \emph{epoch}.
%\emph{epoch}'s \emph{volume velocity}. 
We now introduce the notation used in this paper:
\begin{itemize}
\item $W = \{w_1,w_2,\ldots, w_n\}$: The set of all items appearing in an \emph{epoch}. 
We use term N-grams in this paper.
%Can be terms or term N-grams in this paper.
\item $t_a = \{w_{a1},\ldots, w_{am}\}$: A transaction made up of a set of items. Each transaction has a sequential id, denoted by the subscript letter, derived from its timestamp.
\item $E^{span} = \langle t_a, t_b, \ldots, t_v\rangle$: An epoch of data of a certain span, such as an hour, made up of the sequence of the transactions created within this hour.
\item $s \subset W$: An itemset; any possible combination of items. 
\item $T_s = \{t: t \in E \, and \, s \subseteq t\}$: All transactions containing itemset s. We refer to it as the itemset's postings list.
\end{itemize}

\subsection{Fundamentals}

The two basic operations of frequent itemset mining algorithms are
\emph{candidate generation} and \emph{solution pruning}.
The original Apriori algorithm by Agrawal et al. \cite{agrawal1994fast}
generates candidates of length K (K-itemsets) by merging frequent itemsets of
length (K-1) ((K-1)-itemsets) that differ in only 1 item, usually the last
item given a certain total ordering of items.
By using only the frequent  (K-1)-itemsets for generating candidate K-itemsets,
many possible K-itemsets are implicitly pruned, based on the downward-closure property:
all subsets of a frequent itemset has to be frequent.
This approach still can generate a large number of candidates, especially
in early iterations of the algorithm.
Consider, for example, the generation of candidate 2-itemsets from a database.
This generation requires producing all unordered pairs of 1-itemsets (terms),
after pruning out rare ones with frequency less than the support threshold.
In many domains, including text mining, the number of frequent 1-itemsets is
large enough to prohibit generating a number of candidates in the order of this
number squared.
In text mining, a rather low support threshold has to be used, because the
frequency of terms follow a long-tailed Zipfean distribution.

\section{Basic Algorithm}
\label{sec:lcm}
To overcome the bottleneck of \emph{candidate generation}, many proposed
algorithms take hints from the transaction space rather than operating blindly
in the item space.
Some of these algorithms operate by traversing a data structure representing
the transactions~\cite{han2000mining}; others generate itemsets having
specific properties that help in pruning out more candidates.
In this paper, we expand on LCM \cite{uno2004lcm}, an algorithm based on a
property of a class of itemsets called
\emph{closed itemsets}~\cite{pasquier1999discovering}.
A closed itemset contains any item that is present in all the transactions
containing this itemset.
A formal definition of closed itemsets is given in equation \ref{eq:Closed}: 

\begin{equation}\label{eq:Closed}\mathcal{C} = \{s_c:\, s_c \subset W \, and \,\nexists \, s_d \, where \, s_c  \subset s_d \, and \, |T_{s_c}| = |T_{s_d}|\}\end{equation}

The properties of closed itemsets are as follows:
\begin{enumerate}
\item Adding an item to a closed itemset reduces its support. 
\item A subset of a closed itemset is not necessarily closed, but one or more closed subset must exist for any itemset (formally this could be the empty set, given that any item that appears in all transactions is removed in a preprocessing step). 
\item If a closed K-itemset can be extended any further then one of its supersets will be closed, however not necessarily a (K+1) superset. Itemsets that cannot be extended any further are called \emph{maximal itemsets}, which form a subclass of closed itemsets.
\end{enumerate}

Besides being much smaller than the solution space of frequent itemsets,
the solution space of closed itemsets can be navigated efficiently.
By using an arbitrary total ordering of items, any closed itemset can be
considered an extension of exactly one of its subsets.
Thus, only this subset is extended during candidate generation.
All other subsets do not need to be extended by items that would lead to
the longer closed itemset.
This property is called \emph{prefix preserving closure extension
(PPC-Extension)} and it was proposed and formally proved by
Uno et al.~\cite{uno2004lcm}.
\emph{PPC-Extension} is achieved by following three rules, which we state
after a few definitions to facilitate their statement.
First, an item is \emph{larger/smaller} than another item if it comes
later/earlier in the total ordering.
This terminology comes from the fact that LCM is most efficient if the items
are ordered in ascending order of their frequency.
Second, the \emph{suffix} of an itemset is one or more items 
%whose removal results in
%does not result in
which have to be removed to get
an itemset with greater support.
Notice that they will necessarily be at the end of the itemset,
regardless of the total ordering.
Finally, we call the first item added to the suffix of the itemset its
\emph{suffix head}.
With this terminology, the rules for \emph{PPC-Extentsion} are:
\begin{enumerate}
\item An itemset must be extended by every item that occurs in $T_{itemset}$, 
except items which are \emph{smaller} than its \emph{suffix head};
extending by \emph{smaller} items will lead to closed itemsets already generated in an earlier step. 
%Extension items are added to the itemset in turn in the order given by the total ordering,
%recursively applying PPC-Extension to the extended itemset before adding the next extension item.
\item After adding an extension item, $w_e$, to an itemset, $s$, we add all other items that appear in all transaction containing $s \cup \{w_e\}$.
% ~--~ 
%that is, items whose frequency within  $T_{s\, \cup \, \{w_e\}}$ is equal to $|T_{s \, \cup \, \{w_e\}}|$.
The added items are the \emph{suffix}.
\item If all items in the \emph{suffix} are \emph{larger} than the \emph{suffix head} then add the itemset to the solution. Otherwise, prune this solution branch; all closed itemsets within this branch have already been generated. %, when processing the smallest suffix member.
\end{enumerate}
 
Table \ref{table:PPCExample} is an example of how \emph{PPC-Extentsion} is
used to generate closed itemsets starting from the
\emph{1}-itemset \{`barack'\}.
The upper table enumerates $T_{\{`barack'\}}$.
The lower table shows steps of itemsets generation.
The current itemset along with its frequency is in column 2.
Itemsets marked by an (*) are the closed itemsets that are part of the solution.
The suffix of the itemset is shown in \emph{italic}. 
All possible extension items and their frequencies are in column 3. 
Extension items that are \emph{smaller} than the \emph{suffix head} 
are shown with a line striked through them. 
For each itemset, the extension items are kept 
so that it is known which extension item 
is next in turn to be added.
An item is bolded when its turn to be added has come.
After adding each item, a pass is done on $T_{itemset}$ to 
enumerate and count possible extension items.
To enforce a support threshold infrequent extension items would be removed
after counting, %at this time,
but in this example there is no such threshold.
Finally, column 4 is a comment explaining each step.

%Notice
Table \ref{table:PPCExample} shows that the number of steps is linear in the number of closed itemsets,
and the only additional storage required, besides storage for the documents,
is that required for possible extension items.
Of course, this is a simplified example, but it shows in essence how LCM
achieves its low run time and memory requirements.
We refer the interested reader to Uno et al.~\cite{uno2004lcm} for a
theoretical proof that the algorithm runs in linear time in the number of
closed itemsets,
and that this number is quadratic in the number of transactions.
Performance on a real dataset is shown in section \ref{sec:socmine}.
We proceed by describing how to implement this algorithm. 
%using an inverted index.


%{\tiny

\begin{table*}

\centering
\begin{tabular}{|c|p{6cm}||c|p{6cm}|} \hline
Doc. Id & Document & Doc. Id & Document\\\hline
a& barack \& mitt & b & barack obama \& mitt romney  \\\hline
c& barack obama \& romney & d & barack obama  \\\hline
\end{tabular}
\begin{tabular}{c}
Documents (two per row)\\\\
\end{tabular}
%\scalebox{0.7}{
\begin{tabular}{|c|p{5cm}|p{5cm}|p{5.5cm}|} %[scale=0.3]{| c | l | l | l |}
\hline
Step&Current Itemset&Possible Extension Items&Comments\\ \hline

1& \{barack\} (4) & mitt (2), obama (3), romney (2) & {\small Items in $T_{\{`barack'\}}$ are counted. }\\\hline 
2& \{\emph{barack}\} (4)* & mitt (2), obama (3), romney (2) &  {\small Rule 3: suffix is ordered, emit itemset.  } \\ \hline
3& \{\emph{barack}\} (4) & \textbf{mitt} (2), obama (3), romney (2) &  {\small Items are ordered lexicographically.} \\ \hline
4& \{barack, \emph{mitt}\} (2)* & \textbf{obama} (1), romney (1) &  {\small Extension items reenumerated \& counted.}\\\hline
5 & \{barack, \emph{mitt}, obama\} (1) & romney (1)                       &  {\small Rule 2: `romney' appears in all $T_{itemset}$. } \\\hline
6 & \{barack, mitt, \emph{obama, romney}\}(1)* & & {\small Rule 3: `obama'  is the \emph{suffex head}. } \\\hline
7 & \{\emph{barack}\} (4) & mitt (2), \textbf{obama} (3), romney (2) &  {\small Nothing more to add, back to \{`barack'\}.}\\\hline
8 & \{barack, \emph{obama}\} (3)* & \sout{mitt} (1), \textbf{romney} (2) &  {\small Rule 1: skipping `mitt', adding `romney'  } \\\hline
9 & \{barack, obama, \emph{romney}\} (2)* & \sout{mitt} (1) &  {\small Rule 1: Nothing more to add.  } \\\hline
10 & \{\emph{barack}\} (4) & mitt (2), obama (3), \textbf{romney} (2) &  {\small Back to \{`barack'\}, adding `romney'. } \\\hline
11 & \{\emph{barack}, romney\} (2) &  mitt (1), obama (2) &  {\small Rule 2: add `obama' after `romney'. } \\\hline
12 & \{barack, \emph{romney, obama}\} (2) &  \sout{mitt} (1)  &  {\small Rule 3: suffix is not ordered, prune.} \\\hline
13 & \{\emph{barack}\} (4) & mitt (2), obama (3), romney (2) &  {\small All possible extension items were added. } \\\hline
\end{tabular}
\begin{tabular}{c}
Closed itemsets containing `barack'
\end{tabular}
%}
\caption{Generation of closed itemsets by Prefix Preserving Closure Extension}
\label{table:PPCExample}
\end{table*}
%}


\subsection{Implementation Details}
We show in algorithm~\ref{algo:lcmix} how to implement LCM and PPO-Extension
%based on the search infrastructure. 
using an inverted index.
The inverted index is part of the standard text search infrastructure,
and it is also a  good representation of textual data. 
Other representations that are designed for databases in general
might not be well suited for such sparse data with a large number of items. %,from the nature of .
%For example, the memory requirements of the FP-tree suffers from the sparsity
%of data.
%, since the data structure is succinct only if it can find common
%prefixes within the constraints of its invariant. 
%The possibility of using an inverted index in the implementation 
%of LCM is yet another reason why it is well suited 
%for textual data.

%An inverted index is a good representation of textual data which is
The algorithm takes as input an epoch of data and a support threshold as a
ratio $\alpha$.
It outputs the closed itemsets with support above the threshold.
Along with each itemset in the solution, it also outputs the transactions
in which it occurs~--~which is represented as $\langle items,
T_{itemset} \rangle$.
The symbol $\succ$ denotes that the lefthand side  succeeds the righthand
side in the total ordering of terms. 

\begin{algorithm}
\SetAlgoLined
\LinesNumbered
\SetKwProg{Fn}{Function}{ is}{end}
\KwIn{$\alpha$: Dynamic support ratio}

\KwData{E: Epoch of data}
\KwResult{C: Closed itemsets having support $\alpha$ within E}
C $\gets \{\langle \emptyset, E\rangle\}$ \tcp*{$\emptyset$ is a closed itemset}
X $\gets$ Inverted index of E\;
\ForEach{$w \in X.tokens$}{
	$T_{\{w\}} \gets$ X.postingsList[$w$]\;
	\lIf{$|T_{\{w\}}| \geq \alpha \frac{|E|}{E.span}$}{
		LCM($\{w\}, w, T_{\{w\}}$)
	}
}
\Return{C}\;

\Fn{LCM(s: Current itemset,   $w_{sh}$: Suffix head, \\ $T_s$: Transactions (tweets) containing s)}{
	freq[$1 \ldots w_n$] $\gets$ 0\tcp*{Initialize freq. counts}
	suffix $\gets \{ w_{sh}\}$\;
	\ForEach{$t \in T_s$}{
		\ForEach{$w \in t$}{ 
			freq[$w$]++\;
			\lIf{freq[$w$] $ = |T_s|$}{
				%suffix.add($w$)	
				suffix $\gets$ suffix $\cup\, \{w\}$
			}
		}
	}
	\lIf{$\exists v \in$ suffix: $w_{sh} \succ v$ }{
		\Return
	}
	C $ \gets $ C $ \cup\, \{\langle s \cup {suffix}, T_s\rangle\}$\;
	\ForEach{$v \succ w_{sh}$ and $v \notin $ suffix}{
		\If{freq[$v$] $\geq \alpha \times |E|$}{
			$T \gets T_s \cap v$	\tcp*{Results of query $s$ AND $v$}
			LCM($s \cup suffix \cup \{v\}, v, T$) 
		}
	}
}

\caption{LCM frequent itemsets mining}
\label{algo:lcmix}
\end{algorithm}

\section{Mining social media}
\label{sec:socmine}

Throughout this paper we use data collected from the Twitter public
stream\footnote{https://dev.twitter.com/docs/streaming-apis/streams/public}
since October 1st, 2012. 
We use only tweets written in the Latin script to facilitate tokenization
using white space and other word boundaries.
We collect only the tweet text to avoid reliance on any features specific to a
certain social medium,
and to make the algorithms applicable to other media where text is short  
% such as Facebook or Google+ status updates.
such as comments on a YouTube video.
The only preprocessing we performed was to remove duplicate original tweets
(not retweets) using a Bloom filter.
This filtering removes spam tweets sent by botnets, averaging at 2.86\% of the stream. 
%3352.61069856614 / 116920.214769629

We apply the algorithms to epochs of data, so they are not strictly stream
processing algorithms.
However, we regard the process as mining a sliding window that is moved
forward by time steps of short span.
The time step must be longer than the time needed to mine an epoch of data,
and the performance of our algorithms makes it possible to use a time step
of a few seconds for epochs up to a day long.
Figure \ref{fig:runtimeEpochs} shows the runtime of LCM on epochs of
increasing length, and we will show in section \ref{sec:bounding} that our
extensions do not degrade performance.
The times reported in figure \ref{fig:runtimeEpochs} are averages across all
epochs of the specified length in the last 3 months of 2012, % October, November and December,
using a time step that is half the epoch length.
The \emph{minimum support} threshold we use throughout this paper
is 10 occurrences in the hour of least \emph{volume velocity},
which translates into $\alpha = 0.0002$.

In the rest of this paper we mine epochs of 1 hour span.
The reason behind this choice is our observation that the number of closed
itemsets mined from epochs of span 1 hour or more,
at the same support threshold, remains the same.
This indicates that itemsets mined from shorter epochs of social media text
are not included in the results of mining longer epochs. 
Therefore, the epoch span should be minimized.
However, when the epoch span is shorter than an hour the frequency required
to surpass the support threshold becomes very low,
and number of mined itemsets increases, with many noise itemsets appearing in
the results.

Regardless of the length of the epoch,
many mined itemsets are combinations of function words.
In the next section, we outline how we reduce the number of itemsets and
eliminate the effect of function words by N-gram filtering.

\begin{figure}
\centering
\scalebox{.4} 
{
\begin{pspicture}(0,-5.61)(19.94,5.61)
\rput(9.97,0.0){\includegraphics{runtime_different-epoch-spans_bar-table.eps}}
\rput(9.8,1.44){\includegraphics{runtime_different-epoch-spans_line.eps}}
\usefont{T1}{ptm}{m}{n}
\rput{90.15449}(0.96138316,-0.45039606){\rput(0.6871191,0.225){\LARGE Time in milliseconds}}
\usefont{T1}{ptm}{m}{n}
\rput(10.130039,-0.925){\Large Epoch span in hours}
\end{pspicture} 
}
\caption{Mean runtime at different epoch spans}
\label{fig:runtimeEpochs}
\end{figure}

\subsection{Mining Term N-grams}
\label{sec:ngrams}



\begin{figure*}[htb]
\centering
\scalebox{0.45} 
{
\begin{pspicture}(0,-3.56416)(37.82,3.5241601)
\usefont{T1}{ptm}{m}{n}
\rput(5.6938963,-3.2008398){\LARGE (a) Mean number of closed itemsets}
\usefont{T1}{ptm}{m}{n}
\rput(18.373896,-3.2008398){\LARGE (b) Mean number of distinct items}
\usefont{T1}{ptm}{m}{n}
\rput(32.033897,-3.2208397){\LARGE (c) Mean runtime in milliseconds}
\rput(5.93,0.50416017){\includegraphics{perf_ngramlen1-5_itemsets_supp10+_1hr.eps}}
\rput(18.89,0.5441601){\includegraphics{perf_ngramlen1-5_distinct-items_supp10+_1hr.eps}}
\rput(31.89,0.50416017){\includegraphics{perf_ngramlen1-5_runtime-millis_supp10+_1hr.eps}}
\end{pspicture} 
}
\caption{Effect of the increasing maximum N-Gram length on results of mining of 1hr epochs of data}
\label{fig:ngramsLen}
\end{figure*}


A large number of itemsets are language constructs that bear no information,
such as ``such as''. 
By treating sequential language constructs, and any other multiword expression,
as one item we eliminate a large number of such itemsets.
We can also eliminate itemsets that are made up of all the different fragments
of the language construct along with other items;
for example, \{we, did, it, \#teamobama\} can produce 10 other combinations of
length 2 or more.
There are many measures of association that can be used to detect multiword
expressions, but each measure is good only under certain conditions~\cite{ramisch2012broad,tan2002selecting}.
After preliminary experiments with various measures, we determined that
the best performance
could be obtained by tokenizing the documents into term N-grams with varying N. 

We use term N-gram tokens such that N-grams of high probability are replaced by
(N+1)-grams, resulting in a distribution with no high peaks.  
An N-gram is considered to % of length N
have a high probability if its maximum likelihood estimate from a background
model is higher than a threshold $\eta_N$. 
A background language model built from a long epoch of data 
from the same stream is used for probability estimation.
The tokenization of a tweet starts by tokenizing it into unigrams, then each
unigram of high probability is replaced by two term bigrams~--~by
attaching to it the unigrams before and after it.
We keep replacing N-grams of high probability by two (N+1)-grams 
%repeat this for each N $\ge$ 1,for all N-grams
%with probabilities above the threshold 
until there are no more such N-grams. %; thus, the distribution is relatively flat.

%Notice that the (N+1)-grams replacing an N-gram overlap with each other, with the unigrams that got 
%attached to the N-gram and possibly with other N-grams. 
%However, the  
%We do not prevent overlap, because there is no guarantee that the N-Gram
%created makes any sense.After mining term N-grams we flatten the itemsets to sets of unigrams again.
%This removes overlap between parts of itemsets making it easier to reason
%about how they relate to each other. 
%This is also necessary since an itemset will have different N-gram set
%representations,
%and its postings list is the union of those of the different representations.

The threshold of high probability, $\eta$, is different for each value of N. 
The threshold for unigrams is determined as follows: 
We pick a topical term that is known to  steadily  appear with a rather high frequency, 
and is talked about in all languages; for example, `obama'. 
The maximum likelihood estimate of the probability of the term `obama' 
%within the whole collection of tweets is 
is P(``obama") = 0.0001, which we use as $\eta_1$.
%We use $\eta_1 = P(``obama") = 0.0001$.
At each N, the probability threshold is adjusted to account for the increase
in the number of tokens and the overall increase in the grand sum of counts (caused by overlap).
The adjusted $\eta_N$ is:
\begin{equation}\eta_N = \eta_1 \times \frac{\sum_{\{w:\, w \,\in\, W\, and\, w.length \,\le\, N\}}{freq(w)}}{\sum_{\{v:\, v\, \in\, W \,and \,v.length\,=\,1\}}{freq(v)}}\end{equation}


Figure \ref{fig:ngramsLen} shows the effect of increasing the maximum length
of N-grams from 1 to 5 on 
the number of closed itemsets,
the number of tokens, 
%of length more than 1, 
and the runtime of mining one hour of data.
The values shown are averages across all one-hour epochs in the month of
November 2012.
Figure \ref{fig:ngramsLen}(a) shows that the number of itemsets 
%continues to decrease as expected.
drops by about 90\% by using variable length N-grams,
which overcomes the problem of mining language constructs
while limiting the increase in the sparsity of the data.
%without increasing the sparsity of the data as much as
%if tweets were tokenized into  N-grams.
%while keeping the statistical properties of the data. % better than those of 
%The value of $\eta$ used is 0.0001.
Figure \ref{fig:ngramsLen}(b) shows that the number of distinct items
increases substantially as N goes from 1 to 2, then continues increasing
slightly until it starts decreasing at N=5.
%The large difference between the number of distinct unigrams and bigrams
%is still less than the increase that would have happened
The decrease happens because 
some tweets are tokenized into less 5-grams than 4-grams,
eliminating 4-grams not appearing elsewhere.
%all 4-grams with probability above the threshold
%are parts of tweets from services that use the same text and append a URL,
%such as tweets reporting scores from
%Game Insight\footnote{http://www.game-insight.com/}.
%Such tweets 
%some tweets are tokenized into more 4-grams than 5-grams, 
%whose 4-grams do not appear elsewhere.
%and the 4-grams
%appearing in them do not appear elsewhere.
%Thus, each pair is reduced to one 5-gram.
Figure \ref{fig:ngramsLen}(c) shows that runtime also decreases as N goes
from 1 to 5, since the runtime of LCM  is proportional to the number of closed
itemsets, %CAN DELETE THE FOLLOWING LINE 
and is not affected by the increasing sparsity of data.
%The runtimes in this figure are slightly less than those in
%figure \ref{fig:runtimeEpochs} because they do not include the time taken for
%writing the posting list of each itemset.


%\section{Filtering itemsets}
\section{Selecting Itemsets} %and Ranking
\label{sec:strong}



In the previous section, we discussed our handling of function words,
using a technique that exploits LCM's tolerance to sparsity.
After applying this technique, the average number of itemsets mined from an
hour of twitter data drops from 61,505 to 6,146.
However, there is still redundancy in the itemsets.
For example, figure \ref{fig:sham} illustrates  itemsets related to Donald
Trump's famous tweets in reaction to Obama's victory in
2012\footnote{\url{http://www.huffingtonpost.com/2012/11/07/donald-trump-election-revolution_n_2085864.html}}.  
Each area in the figure represents the transactions containing the itemset
formed by concatenating the items in all intersecting ellipses. 

The closed property of an itemset is easily satisfied by an itemset
created through the modification of 
transactions that contain another closed itemset.
For example, if a transaction containing a closed itemset
is modified by removing one of its items,
another closed itemset with enough support is immediately formed.
%resulting in a two closed itemsets instead of one.
While an update operation is not supported in the model of frequent itemset mining, 
a similar effect happens when people retweet, 
or even when they are writing original posts about 
a certain fine grained topic.
In case of retweeting, people actually start from the original tweet and 
then remove parts of it to make space for their content,
or to keep only the part they want to emphasize. 
For example, in figure~\ref{fig:sham}  the most discriminative words are ``sham, and, travesty''
which are quoted 
%along with Donald Trump's user name 
in most of the retweets.
We can imagine that for each fine grained topic there is also a base post
that represent the information or ideas within the topic. 
People make posts about the fine grained topic 
by selecting parts of the base post
and adding their own thoughts.
This results in many closed itemsets 
about the topic that
are trivially different from each other.
%We now propose a condition that exploits this 

The fact that any \emph{maximal} itemset is a closed itemset
is another \emph{weakness}. % of the closed condition.
%when used for mining social media text.
%If an item is added to a number of retweets higher than the support threshold
%it results in a \emph{maximal} itemset.
%The itemset will be closed by definition even if the support threshold
%is much lower than the total number of retweets
If a closed itemset is expanded by certain items a number of times
over the support threshold, this results in a maximal itemset
which is closed by definition.
%If the support of the original itemset is much higher than the support threshold,
%the newly created itemset might not 
%but might not be carrying 
%if the support 
%threshold is much lower than the 
%Now consider that in social media a low support threshold has to be used.
Now consider a tweet that is retweeted hundreds of times, 
%resulting in some closed itemsets,
%and different groups of people append the same words to it,
and a small group of people appends the same words to it.
%but no group is considerably large relative to the number of retweets.
Since  a low support threshold has to be used when mining text,
%this will result in many maximal itemsets that 
this will result in a maximal itemset that
might not be adding any information to the closed itemset.
For example, in figure~\ref{fig:sham} a number of people
talked about ``hair'' along with ``sham, and, travesty''. 
The number is higher than the support threshold,
but it is relatively small within the topic.
%While amusing, this addition is not important.

We propose two conditions that are not as easily satisfied as the closed
condition for selecting itemsets.
The two conditions build on the concept of \emph{association rule confidence}.
%Confidence is the basic property used for association rules mining,
%and it is used in the definition of $\delta$-free sets \cite{boulicaut2003free}.
Mining itemsets based on the confidence of rules they induce has long been
recognized as a method for finding
``interesting patterns'' \cite{cohen2001finding},
but since this property is not anti-monotone 
it cannot be directly used.
%a variation has to be used
%(for example, \emph{all confidence}). % \cite{kim2004ccmine}
The confidence of an assocation rule that the presence of an itemset,
$s_{j}$, implies the presence of another itemset, $s_i$, is  defined as:
\begin{equation}\label{eq:conf}conf(s_j \rightarrow s_i) = \frac{|T_{s_i} \cap T_{s_j}|}{|T_{s_j}|}\end{equation}


%%%%%%%%%%%%


\begin{figure}
\centering
%\scalebox{0.7} 
\resizebox{7cm}{3.5cm}
{
\begin{pspicture}(0,-2.649212)(9.077745,2.95)
\psframe[linewidth=0.04,dimen=outer](8.843161,2.95)(0.2431605,-2.61)
\usefont{T1}{ptm}{m}{n}
\rput(4.103317,1.675){sham, and, travesty}
\usefont{T1}{ptm}{m}{n}
\rput(1.7484828,2.575){@realdonaldtrump}
\pscircle[linewidth=0.04,dimen=outer](4.1031604,0.01){2.28}
\rput{55.644855}(2.9522903,-5.6005197){\psellipse[linewidth=0.04,dimen=outer](6.782275,-0.0031560312)(1.9103949,2.0626228)}
\rput{34.21703}(0.13301769,-1.7201583){\psellipse[linewidth=0.04,dimen=outer](2.8607624,-0.6440031)(2.908739,0.81)}
\pscircle[linewidth=0.04,dimen=outer](4.5031605,-0.33){1.58}
\usefont{T1}{ptm}{m}{n}
\rput(4.527965,-0.865){not,}
\pscircle[linewidth=0.04,linestyle=dashed,dash=0.16cm 0.16cm,dimen=outer](2.6031604,0.81){0.48}
\usefont{T1}{ptm}{m}{n}
\rput(1.0566566,1.455){fraud}
\pscircle[linewidth=0.04,linestyle=dashed,dash=0.16cm 0.16cm,dimen=outer](1.0631605,1.45){0.46}
\usefont{T1}{ptm}{m}{n}
\rput(2.623102,0.815){hair}
\pscircle[linewidth=0.04,dimen=outer](7.3231606,-1.35){1.02}
\usefont{T1}{ptm}{m}{n}
\rput(7.337262,-1.025){the, world, }
\usefont{T1}{ptm}{m}{n}
\rput(7.264635,-1.325){is, laughing}
\rput{34.21703}(0.8391447,-4.3701773){\psellipse[linewidth=0.04,dimen=outer](7.5185595,-0.8219682)(1.3607179,0.95923364)}
\rput{34.21703}(0.3818179,-4.274056){\psellipse[linewidth=0.04,dimen=outer](7.133755,-1.5167968)(1.2777647,0.86427283)}
\usefont{T1}{ptm}{m}{n}
\rput(7.207018,-1.645){at}
\usefont{T1}{ptm}{m}{n}
\rput(8.076705,-0.165){us}
\usefont{T1}{ptm}{m}{n}
\rput(6.371842,-2.105){you}
\usefont{T1}{ptm}{m}{n}
\rput(1.4102015,-1.625){elections}
\usefont{T1}{ptm}{m}{n}
\rput(6.9122915,1.255){rt}
\usefont{T1}{ptm}{m}{n}
\rput(4.4461293,-1.265){democracy}
\end{pspicture} 
}

\caption{Closed, distinct and strongly closed sets}
\label{fig:sham}
\end{figure}


\subsection{Distinct Itemsets}


The \emph{distinct} condition is a novel strengthening of the closed condition
so that it is not as easily satisfied. %violated by trivial differences.
We define a \emph{distinct} itemset as a closed itemset whose frequency
comprises more than a certain proportion of the frequency of its least
frequent subset~--~we call this its \emph{parent} itemset. 
%This condition chooses closed itemsets which substantially violate the closed
%condition of their subsets.
The proportion is a parameter, $\kappa$, that controls the selectivity of the
\emph{distinctiveness} condition.
%The proportion is a parameter, $\kappa$, that controls the selectivity 
%of the condition,
%or the \emph{distinctiveness} of itemsets selected by the condition.
This can be interpreted as selecting itemsets which are implied by a subset
with confidence greater than $\kappa$.
Formally, the set of \emph{distinct} itemsets, $\mathcal{D}$,
is defined as follows:
\begin{equation}\mathcal{D} = \{s: s \in \mathcal{C} \, and \, \exists \; s_{parent} \subset s \; where \; \frac{|T_{s}|}{|T_{s_{parent}}|} \ge \kappa 
\}
\end{equation}


In figure \ref{fig:sham}, \emph{distinct} itemsets are illustrated in solid
lines, and closed itemsets that do not satisfy the distinctiveness condition in dashed lines.
It is clear from the figure that considerable redundancy remains in
\emph{distinct} itemsets.

\subsection{Strongly Closed Itemsets}
To overcome the redundancy in \emph{distinct} itemsets 
we merge similar ones
into \emph{strongly closed} itemset clusters.
This will also filter out any itemset 
that does not belong to a particular topic
since it will not be part of a cluster.
The similarity of a \emph{distinct} itemset,
$s_d$, and another distinct itemset, $s_c$,
is measured as the overlap of the \emph{transactions} containing both of them
with the transactions containing $s_c$. 
A \emph{distinct} itemset is clustered with another itemset if one exists
such that the overlap exceeds a similarity threshold,
which we take to be $1-\kappa$ (the indistinctiveness of $s_d$ from $s_c$).
If more than one satisfies this condition,
the  \emph{distinct} itemset is clustered with the 
one having the highest overlap ratio.
When  a \emph{distinct} itemset is clustered with an itemset that is already
part of a cluster, the \emph{distinct} itemset is added to the existing cluster.
Finally, the \emph{strongly closed} itemset is the union of all cluster members,
and its support is the size of the union of their postings lists.
We define the  desired clustering  and the strongly closed itemset
represented by each cluster as follows:
\begin{align*}\label{eq:strongClosedFormal}
\mathcal{R} = \{r: r = \bigcup_i{(s_i, s_j)}\; where\; s_i \in \mathcal{D} \, and \, s_j \in \mathcal{D} 
\\\,and\; \forall_{(s_i,s_j)} \, s_j = \textbf{argmax}_{s_k} conf(s_k \rightarrow s_i) \\\,and \;conf(s_j \rightarrow s_i) \ge (1-\kappa)
\\\, and\;( s_j = r.centroid\; or \; (s_j, r.centroid) \in r )\}
\end{align*}
\begin{equation}S_l = \{w:\, w \in \bigcup_{(s_i, s_j) \in r_l}{s_i} \; where \; r_l \in \mathcal{R}\}\end{equation}

Confidence is not a symmetric measure,
but our proposed clustering makes use of the order
in which PPC-extension generates itemsets 
to determine which itemset should be the antecedent 
and which should be the consequent.
If PPC-extension uses a total ordering 
of items that is non-increasing in item frequency,
then 
an itemset generated earlier 
should be the antecedent, and
an itemset generated later
should be the consequent.
This builds upon the use of
variable length N-grams
as items,
so that the most frequent items
are not function words.
In that case, following the non-increasing order 
of item frequency leads to mining itemsets related 
to a certain topic close to each other.
Notice that PPC-Extension is equivalent to a depth first traversal 
of a hypothetical prefix tree of closed itemsets.
That is, closed supersets of each itemset are generated before 
moving to an itemset that is not a superset. 

The most discriminative itemset from each topic 
%(according to users' selection) 
is mined 
before other itemsets from the same topic.
%because it is composed of items having the highest support within its topic.
If an itemset branches out into supersets related to two topics 
or two opinions within one topic, this itemset is 
mined before itemsets from 
either of the two branches. It should not 
be clustered with either of the branches.
For an itemset to belong to a topic, it must contain a set of topical words. 
Since we are processing items in descending order of frequency, 
then itemsets containing multiple topical words are generated
when mining the supersets of the itemset containing only 
the most frequent of them.
%Thus, itemsets belonging to a topic
%will be mined after the topic's most discriminative itemset,
%even if they are not its superset.
According to these observations, 
the \emph{topical} similarity between itemsets
can be indicated by
the confidence of 
the association rule that
an itemset generated earlier implies 
an itemset generated later.

%%%%%%%%%%%%%%%%

%This clustering scheme selects the cluster that contains the itemset which
%maximizes the confidence of the rule $conf(s_j \rightarrow s_i)$,
%with a lower bound on the overlap to maintain distinctiveness. 
This clustering can be implemented efficiently using techniques similar to
the ones proposed by Bayardo et al. \cite{bayardo2007scaling}.
The main ideas are to limit the comparisons to a few candidates,
and to terminate the comparison early if the similarity threshold will not
be met.
In our case, the postings lists are longer than the itemsets,
so we generate candidates for comparison by calculating similarity between
itemsets.
When calculating the similarity between two postings lists,
we can terminate early if the difference exceeds the maximum difference
permissible to achieve a similarity of $1-\kappa$,
which can be derived from equation \ref{eq:conf}. 

Algorithm~\ref{algo:alliance}  shows a possible implementation.
For each itemset, $s_i$, we find the itemsets produced before it
and overlapping with it in one or more items.
Then we find the candidate, $s_c$, that maximizes
$conf(s_c \rightarrow s_i)$ such that the confidence exceeds $1-\kappa$.
Notice that confidence is not a symmetric measure,
and we only check the confidence of the rule that the clustering candidate
implies the itemset. 


\begin{algorithm}[t]
\SetAlgoLined
\LinesNumbered
\KwIn{$\kappa$: Minimum distinctiveness threshold} 
\KwData{$\mathcal{C}$: Closed itemsets produced by LCM}
\KwResult{$\mathcal{R}$: Strong closed itemset clusters}
\For{$i \gets 2$ to $|\mathcal{C}|$}{
	$C \gets \{s_c: s_c \in \mathcal{C} \, and \, c < i \, and \, |s_c \cap s_i| > 0\}$\;
	$P \gets \{s_p: s_p \in \mathcal{C} \, and \, p < i \, and \, s_p \cap s_i = s_p\}$\;
	$s_p \gets \textbf{argmax}_{s_p \in P}\frac{|T_{s_i}|}{|T_{s_p}|}$ \tcp*{Direct parent}
	\uIf{$s_p = null$ \textbf{OR} $\frac{|T_{s_i}|}{|T_{s_p}|} < \kappa$}{continue\tcp*{Not a distinct itemset}}
	%	$s_m \gets s_i$  \tcp*{Cluster centroid, initially self}
	%$s_m \gets null$  \tcp*{Cluster centroid}
	$s_m \gets null$, $maxConf \gets 0$ \tcp*{Best clustering}
	\ForEach{$s_c \in C$}{
		$\Delta \gets (1 - (1-\kappa)) |T_{s_c}|$ \tcp*{Maximum difference}
		$\delta \gets$ difference($T_{s_c},T_{s_c \cup s_i},\Delta$) \tcp*{Stops early}
		
		\If( \tcp*[f]{Confidence $\ge$ threshold}){$\delta \le \Delta$}{ 
			$conf \gets \frac{|T_{s_c}| - \delta}{ |T_{s_c}|}$\;
			\If{$conf > maxConf$}{
				$s_m \gets s_k$ \tcp*{Best merge candidate}
				$maxConf \gets conf$ \;
			}
		}
	}
	\If{$s_m \ne null$}{
		\lIf(\tcp*[f]{New cluster}){$\mathcal{R}[s_m] = null$}{$\mathcal{R}[s_m] \gets s_m$} 
		$\mathcal{R}[s_i] \gets \mathcal{R}[s_m]$ \tcp*{Add $s_i$ to the cluster}
		$\mathcal{R}[s_m].itemset \gets \mathcal{R}[s_m].itemset \cup s_i \cup s_m$\;
		$\mathcal{R}[s_m].postingsList \gets \mathcal{R}[s_m].postingsList \cup s_i.postingsList \cup s_m.postingsList$\;
	}}
\Return{$\mathcal{R}$}\;
\caption{Forming strongly closed itemset clusters}
\label{algo:alliance}
\end{algorithm}



\subsection{Performance Analysis}
\label{sec:bounding}



Unlike the original LCM algorithm, filtering low confidence itemsets requires
us to keep mined results in memory to calculate the confidence of newly
generated ones.
The memory requirement is not large because the number of itemsets 
averages at about 6,000.
In our experience with the Twitter data it was enough to keep only a buffer
of 1,000 itemsets, when mining an hour long epoch.
This is based on the observation that the number of positions 
between an itemset and a clustering candidate 
is 1,973.11 on average, with a standard deviation of 1,443.64. 
The large standard deviation indicates that
candidates are either found close to the itemset
(less than 500 positions behind),
or in a position that is several thousands 
of itemsets behind. 
%Thus we keep only 1,000 itemsets.

Following is an analysis of
%We analyze
the performance of the filtering conditions proposed,
%We evaluate the efficiency of our proposed filtering and clustering methods 
in terms of the number of itemsets after their application,
and the runtime overhead they add on top of 
the runtime of the LCM algorithm.
The experiments were run using a single threaded Java implementation of
algorithm \ref{algo:alliance},
running on a 1.4GHz processor with 2MB of cache.
%by applying
We apply the algorithm to the mining results of all 
one-hour long epochs in the Twitter data.
Before filtering, the average number of itemsets mined 
from an hour-long epoch is
2,439.17 closed itemsets of length 2 or more;
that is, excluding itemsets that are merely a frequent item. 



%The number of length 2 or more maximal itemsets is 1831.92,
%which we provide just as a reference to the result of a basic way of filtering.

Figure \ref{fig:kappa} show the effect of varying $\kappa$ on the mean number
of \emph{distinct} and \emph{strong closed} itemsets.
The number of \emph{distinct} itemsets drops as the distinctiveness threshold
increases.
On the other hand, the number of \emph{strong closed} clusters %formed increases
as the similarity (indistinctiveness) threshold decreases.
The dashed line shows that the number of unclustered distinct itemsets reaches
zero at $\kappa=0.5$, explaining why the number of clusters changes very
slightly after that. 
We use $\kappa = 0.25$ in the remainder of the paper,
which is an arbitrary choice based on the definition not the data.
The average number of itemsets at this value of $\kappa$
averages at 139.1 \emph{strongly closed} itemsets,
and 85.38 \emph{distinct} itemsets. 
%(\emph{strongly closed} and unclustered
%\emph{distinct}) mined from one-hour epochs at this value of
%$\kappa$ is only 224.48, which is about 10\% of the number of closed itemsets.



\begin{figure}
\scalebox{.5} 
{
\begin{pspicture}(0,-4.16)(16.6,4.16)
\rput(8.3,0.0){\includegraphics{kappa_effect.eps}}
\end{pspicture} 
}
\caption{Effect of changing $\kappa$ on mining results }
\label{fig:kappa}
\end{figure}



Figure \ref{fig:lcmvsfpzhu} shows the total runtime of the LCM algorithm plus
filtering based on the distinct condition and clustering into strong closed
itemsets at different epoch spans.
The runtime of LCM alone is also plotted for reference.
We also plot the performance of another frequent itemset mining algorithm,
FP-Zhu \cite{grahne2004reducing}, which was the runner up at
FIMI 2004~\cite{DBLP:conf/fimi/2004}.
We include it to show that our extensions do not degrade the performance of
LCM even in the context of competitions.
The y-Axis is in logarithmic scale to keep the scale of the plot suitable for
seeing slight differences.
The output of LCM is the input to the filtering and clustering step,
so it is affected by the number of closed itemsets produced.
This explains why it takes slightly longer time for clustering results from
the 15-minute epoch and then takes 
a constant time for epochs of a longer span. 

These runtimes are achieved by using a buffer of 1,000 itemsets,
which is the same setting we use when evaluating the quality of the synopsis
in the next section.
If all itemsets are kept in memory the runtime increases slightly and
averages at 5.2 seconds for filtering a one-hour epoch.

\begin{figure}
\centering
\resizebox{9cm}{4.25cm}
%\scalebox{0.6} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-4.52)(16.408476,4.52)
\rput(7.52,0.0){\includegraphics{runtime_lcm-lcm+filter-fpzhu_seconds.eps}}
\usefont{T1}{ptm}{m}{n}
\rput(14.1,-3.85){\large ($\kappa = 0.25$)}
\end{pspicture} 
}
\caption{Runtime of itemset mining alone and with filtering and clustering at different epoch spans}
\label{fig:lcmvsfpzhu}
\end{figure}

\section{Temporal Ranking}
\label{sec:rank}
The previous filtering steps reduce the number of itemsets to 
under 10\% of their original number.
In this section, we present a method for ranking itemset clusters 
%according to
%their novelty when compared to other time periods.
%These itemsets can then 
such that they
be presented to users in rank order
as a summary of events in the epoch.
The ranking can also be
%providing a synopis of events in the epoch, or 
used as input to additional search or
summarization steps.

We measure temporal novelty compared to 
a longer period of time 
leading up to the mined epoch
--~a background model.
A good indicator of novelty is the point-wise Kullback-Leibler Divergence (KLD)
between an itemset's probability in the current epoch and in the background model.
The KLD of the probability of an itemset, $s_i$, in the background
model, $Q$, from the current epoch's model, $P$, can be considered as
the Information Gain, IG: 
\begin{align}IG(P(s_i),Q(s_i))  &= - (H(P(s_i)) - H(P(s_i),Q(s_i))) \notag\\ &= \sum{P(s_i) \log{P(s_i)}} - \sum{P(s_i) \log{Q(s_i)}} \notag\\ &= KLD(P(s_i)||Q(s_i))\end{align}

To calculate the collective IG of a strongly closed itemset cluster,
we have to take into account that the itemsets of the cluster are not independent. 
For simplicity we will consider only the pairwise dependence between 
every itemset and the smallest common subset, $s_{min}$.
We also normalize by the size of the cluster, $m$, to avoid favouring large clusers. Hence, our ranking formula for
strongly closed itemsets:
\begin{align}\label{eq:avgIG}score(r) &= IG(P(s_{min}), Q(s_{min})) \notag\\ &+ \frac{ \sum_{ j = i_1..i_m } { IG(P(s_j|s_{min}), Q(s_j|s_{min})) } } { m } \end{align}


\subsection{Empirical Evaluation}
\label{sec:empirical}
We now show examples of the synopses produced by ranking
the \emph{strongly closed} itemsets. % as discussed in section~\ref{sec:rank}.
The examples will serve as our assessment of the quality of 
the synopses. 
We make an effort to choose epochs from time periods when 
it is known what people should be talking about. 
%In section~\ref{sec:electionsday}, 
We show how the 2012 US presidential elections day is summarized by our methods. %hour-by-hour
We also compare our summary to the one produced by a state of the art algorithm,
which generates better quality summaries than other summarization algorithms based on itemsets~\cite{mampaey2011tell}. 
We also investigate the synopses of hours selected by the aid of Google Trends\footnote{\url{http://www.google.ca/trends}},
assessing which popular queries were or were not reflected in our synopses.

\subsubsection{U.S. Presidential Elections Day}
Table \ref{table:nov6} shows the top 3 \emph{strongly closed} itemsets
from one-hour epochs on the elections day.
The itemsets shown are the first appearances of the most interesting itemsets;
that is, an hour is shown only if its top 3 feature novel interesting itemsets.
%\footnote{The top 50 itemsets in the hours from 5 AM on the 6th till almost 9 AM on the 7th are available for download at \url{http://plg.uwaterloo.ca/~yaboulna/thesis_results/twitter_synopses/elections-day_nov6-0500_nov7-0850_top50-strongly-closed.txt}}.
The first column is the beginning of the hour, EST time.
The second column is the top itemsets, 
reordered and punctuated to be meaningful phrases 
by looking at a few tweets from the postings list of each itemset.
The third column is a commentary to explain the itemsets,
also composed constructed according to tweets in the postings lists.
The fourth column is the rank of an equivalent itemset 
in the top 30 itemsets mined by the MTV algorithm. % if one exists.

Table~\ref{table:mtv} shows the top 3 itemsets picked by MTV, 
for hours when they include interesting itemsets.
Each interesting itemsets has
an equivalent \emph{strongly closed} itemset is 
present in the top 50\footnote{\url{http://
blinded.blinded.com/blinded/
%plg.uwaterloo.ca/~yaboulna/
thesis_results/twitter_synopses/elections-day_top50.txt}}.
%nov6-0500_nov7-0850_top50-strongly-closed.txt}}.
Its rank is in the second column.

%We do not know if all the top ranked \emph{strongly closed} itemsets 
%would also be picked by MTV if we use it to mine more itemsets.
%The space and time requirements of MTV
%becomes prohibitively large when the number
%of itemsets requested is increased~--~
%it takes more than an hour to mine 
%the top 50 itemset from an hour long epoch.


\begin{table}
%\vspace{-3cm}

%\thispagestyle{empty}
\begin{center}
\footnotesize
\def\arraystretch{1.2}
\begin{tabular}{|p{0.7cm}|p{2.5cm}|p{4cm}|c|}
\hline
\textbf{Time} & \textbf{Itemset} & \textbf{Explanation} & \textbf{M} \\ \hline
\multirow{3}{*}{\texttt{08:30}}
& \em get out and vote & Encouraging participation & 17 \\ \cline{2-4}
& \em \scriptsize happy elections day & Congratulations & 27 \\ \cline{2 - 4}
& \em it's elections day & Excitement  & - \\ \hline

\multirow{3}{*}{\texttt{09:30}}
& \em if romney wins & Reflecting about possiblities & - \\ \cline{2 - 4} %dreading the thought (according
& \em of your ballot & Telling people to stop sending pictures of ballots & - \\ \cline{2-4}
& \em in line to vote & Tweeting while in line  & 28 \\ \hline

%\multirow{3}{*}{\texttt{15:00}}
%& \em Borusia Dortmnd Real Madrid 2 & Score of football match; no events related to elections & 17 \\ \cline{2 - 4}
%& \em incroyable talent & A TV show from France & - \\ \cline{2-4}
%& \em City Ajax 2 & Another football match & - \\ \hline

\multirow{3}{*}{\texttt{19:30}}
& \em Linda  McMahon		&  She loses CT senate race & - \\ \cline{2 - 4}
& \em Florida is so ... & tight or close; the vote count  & - \\ \cline{2-4}
& \em \small Romney is winning & Speculations about results  & - \\ \hline %He did take the lead for a while


\multirow{3}{*}{\texttt{20:00}}
& \em New Hampshire &  Obama won in NH & 9 \\ \cline{2 - 4}
& \em Obama to win / is winning & Two itemsets speculating  & - \\ \cline{2-4}
& \em too close to call & The results are still not clear & 22 \\ \hline


\multirow{3}{*}{\texttt{21:00}}
& \em Ohio and Florida &  Obama won in 2 more states & - \\ \cline{2 - 4}
& \em Sarah Palin & Sarah Palin  on Fox news  & - \\ \cline{2-4} %(uh oh!)
& \em concession speech & Todd Akin's  speech & - \\ \hline


\multirow{5}{*}{\texttt{22:00}} 
& \em The electoral college &  Republicans complaining about the voting system  & 9 \\ \cline{2 - 4}
& \em President of the United States & Barack Obama is the 44th president of the USA   & 8 \\ \cline{2-4} %(uh oh!)
& \em who voted for & Taking things personally & - \\ \cline{2-4} % 
& \em once you go black you never go & A popular culture reference, missing ``back''& 15 \\ 
%\cline{2-4} %(uh oh!)
%& \em my president is black & Americans proud about the equality of opportunities & - \\
 \hline % 

\multirow{3}{*}{\texttt{22:30} }
& \em @realdonaldtrump this elections ... [complete tweet]	& One of Donald Trump's famous tweets. This cluster merges 26 \emph{distinct} itemsets. & 1  \\ \cline{2-4}
& \em concede to & The loser has to concede  & - \\ \cline{2-4} %(uh oh!)
& \em Obama won. I ... & Reactions to the result & - \\ \hline % 

\multirow{3}{*}{\texttt{23:00} }
& \em same sex marriage	& Some states legalized same sex marriage & 27  \\ \cline{2-4}
& \em for recreational use & CO and WA legalized weed for recreational use   & 21 \\ \cline{2-4} %(uh oh!)
& \em acceptance speech & Anticipation for the speech & 26 \\ \hline % 

\multirow{3}{*}{\texttt{00:30} }
& \em Delivered, sealed, signed & Stevie Wonder's song used in Obama's campaigns & 10  \\ \cline{2-4}
& \em The best is yet to come & Quote from Obama's acceptance speech  & 2 \\ \cline{2-4} %(uh oh!)
& \em with the flag in her [hair]	&  During the speech, attention goes to a woman appearing behind Obama on TV! & 14 \\ \hline %  \cline{2-3}
						
\end{tabular}
\end{center}
\vspace{-5mm}
\caption{Top 3 \emph{strongly closed} itemsets for hours in US presidential elections day (November 6, 2012)} %  showing how the day unwound (or 5)
 \label{table:nov6}
\end{table}

 \begin{table}
 \begin{center}
\small
\def\arraystretch{1.2}
\begin{tabular}{|p{7.5cm}|c|}

\hline
\textbf{Itemset} (with [tweet id] if itemsets is a whole tweet) & \textbf{S} \\ \hline

@realdonaldtrump,this,elections,...[266035509162303492] & 1
\\ \hline
 %This election is a total sham and a travesty. We are not a democracy! 
we, re, all, in, this, together, ..., bo [266031109979131904]  & 22 \\ \hline
 % That's how we campaigned, and that's who we are. Thank you. -bo
@realdonaldtrump,our,country,is,...[266037143628038144]  & 24 \\ \hline
%Our country is now in serious and unprecedented trouble...like never before.
\specialrule{.1em}{.05em}{.05em} 
%\hline

URL,and,autotweet,checked,followed,me,today,unfollowed & N/A \\ \hline

house,of,representatives,shouldnt,...[266040877552656385] & 15 \\ \hline
% House of Representatives shouldn't give anything to Obama unless he terminates Obamacare.
 URL,\#android,\#androidgames,\#gameinsight & N/A \\ \hline
\specialrule{.1em}{.05em}{.05em} 
%\hline 

URL,and,autotweet,checked,followed,me,today,unfollowed & N/A \\ \hline
URL,\#android,\#androidgames,\#gameinsight  & N/A \\ \hline
@barackobama,@ryanseacrest, what, was, your, first, words, in, reaction, to, re, election, 2 & 36 \\ \hline 
%\hline 
\specialrule{.1em}{.05em}{.05em} 
URL,and,autotweet,checked,followed,me,today,unfollowed & N/A \\ \hline
the, best, is, yet, to, come & 2 \\ \hline
 URL,\#android,\#androidgames,\#gameinsight & N/A \\ \hline
 

\end{tabular}
\end{center}
\caption{Top 3 picked by the MTV algorithm}
 \label{table:mtv}
\end{table}

\section{Conclusion and future work}
\label{sec:concfut}
We have proposed a method for efficiently creating temporal synposes of social
media streams, based on a frequent itemset mining algorithm that is suitable
for sparse data, LCM.
Our method summarizes an hour of Twitter data (116920 tweets on average) into
224 itemsets in 1945.68 milliseconds on average,
and scales for longer epochs of data.
The direct application of LCM on one-hour epochs of Twitter data results in
an average of 61505.16 closed itemsets and takes 2506.58 milliseconds on
average.
The improvement is due to the following contributions: 
(1) strengthening  the closure condition such that it selects an itemset only if
it is distinctively different from its subsets and other itemsets, and 
(2) using variable length N-grams to mitigate the effect
of the skewness of the frequency distribution of unigrams.
The distinctiveness between two itemsets is based on a parameter, $\kappa$, which controls tolerance to redundancy. %Even though the use or variable length N-grams

Another important contribution is a method for ranking itemsets based on their temporal novelty.
The top 3 itemsets from the hours of election day and another less eventful
day shows that the synopses captures important events, and might reasonably
be directly presented to users as a summary.
A possible future direction is to use itemsets that appear as a sequence for
building extractive coherent summaries of the social media stream at
different times.

We aim to exploit the synopses for temporal query expansion in our future work. Terms from itemsets relevant to a query (or occurring in a relevant document)
can be used for query expansion, thus acting as precomputed results of 
pseudo-revelance feedback.
We also wish to explore ways to make use of the temporal signal during mining,
such as when calculating similarity during clustering.

 

%\section{Acknowledgements}
% No Acknowledgments yet...
%This paper was funded in part by the Google Focus Award. Right?? What else should we say here? I am writing to fill some space and reserve it for the actual acknowledgements.
%% Thank YOU!


\small
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{younos_wsdm2014}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns

\end{document}
