% This is "sig-alternate.tex" V1.9 April 2009
% This file should be compiled with V2.4 of "sig-alternate.cls" April 2009
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.4 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.4) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.9 - April 2009

\documentclass{sig-alternate}
\usepackage[normalem]{ulem}
\usepackage[usenames,dvipsnames]{pstricks}

%\usepackage{epsfig}
%\usepackage{pst-grad} % For gradients
%\usepackage{pst-plot} % For axes

\usepackage{mathtools}

\usepackage{algorithm2e}

%\usepackage[pdflatex]{graphicx}
%\usepackage{caption}
%\DeclareCaptionType{copyrightbox}
%\usepackage{subcaption}


\usepackage{multirow}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{CIKM}{'13}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

%\title{Adapting Frequent Itemsets Mining for Social Media Text by Strengthening the Closed Property}
\title{Strong Closure and Novelty Ranking for Efficiently Mining a Temporal Synopsis of Social Media }
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.
\maketitle
\begin{abstract}
Abstract
\end{abstract}

\section{Introduction}

The nature of text in social media poses a challenge when applying traditional search and text mining algorithms. Text in social media is usually short, lacks context and structure, and is created at a very high rate. The sheer volume of social media streams mandates the use of efficient algorithms, and the short length of individual documents makes it possible to apply Frequent Itemsets Mining. This family of algorithms is very fast and efficient, however it is not readily suited for application on text. The problem with Frequent Itemsets Mining is the large number of itemsets it produces.
%, specially that most of them are not interesting. 
Actually it was originally proposed as a preliminary stage to Association Rules Mining, which sifts through the numerous itemsets and produces a smaller number of association rules. The number of itemsets produced can be reduced by setting a higher threshold on the minimum frequency of an itemset. However, in text this threshold has to be kept low because frequencies follow a long tailed Zipfean distribution. Also, the head of the distribution is made up mostly of stop words; a situation that doesn't happen in the domain of market basket data since the packaging of each item is not enlisted in the receipt!
%The first type of uninteresting itemsets is the language constructs that bear no information, such as ``such as'', and itemsets that are made up of all the different fragments of the language construct along with other items. The latter type is an artifact of itemset mining when the final itemset has such a language construct within it; for example, \{we, did, it, \#teamobama\}.
%It produces a very high number of itemsets most of them are uninteresting language constructs.
%The number of itemsets it produces is exponential in the number of items,  
%For example, one would expect that \{obama, won\}, \{sham, and, travesty, @realDonaldTrump\} and \{flag, in, her, hair\}  are  itemsets mined from Tweets posted on November 6th, 2012 using terms as items. They appear frequently in the last couple of hours of the US elections day, but they are not as frequent as \{of, the\} and other English language constructs. 
Even if a maximum frequency threshold is set, risking to filter out important itemsets, a lot of non-English language constructs will be mined because the proportion of posts in English is much higher than other languages. In this paper we propose methods for adapting Frequent Itemset Mining to be effective on social media text, without degrading its efficiency. %or relying on features of a particular language.

Unlike trending topics\footnote{http://blog.twitter.com/2010/12/to-trend-or-not-to-trend.html}  \cite{mathioudakis2010twittermonitor}, the results of Frequent Itemset Mining include itemsets that have high frequency because of sustained relatively high interest as well as a spike of interest. 
% GOES TO RELATED WORK Unlike bursty events detection methods \cite{klienberg2002},
%Frequent itemsets include ``trending topics'', as well as topics that have sustained interest for longer periods of time without sudden peaks. We use the term ``trending topics'' as defined by Twitter in a blog post explaining why a topic about WikiLeaks wasn't trending as was expected by internet activists \footnote{http://blog.twitter.com/2010/12/to-trend-or-not-to-trend.html}. The blog post explains that a topic is trending if a sudden peak is detected, and links this to the change in the velocity of the conversation. Since the number of posts associated with such a trending topic must be high, Frequent Itemsets Mining will detect it. Furthermore, it detects topics that have a steady high volume of posts.
The mined itemsets provide the vocabulary associated with events and can be used in various ways for search and summarization. For example, the collection of mining results of different epochs of time can be used for temporal query expansion \cite{choi2012temporal}. The mining results of each epoch can be treated as a document, so that the probability of an epoch conditioned on a query can be estimated as the probability of this document on the query using any pseudo-revelance feedback method. Further, terms from individual itemsets relevant to a query (using any relevance ranking) can be used for query expansion, thus acting as precomputed results of a simple version of pseudo-revelance feedback. 
% after applying appropriate weights to items.
%Liken to Rosie Jones Temporal profiles?? 
% Mention CMU and/or Glasgow's work that used pseudo-relevance feedback for Microblogs?
% FUTURE WORK: For summarization, itemsets that appear as a sequence can be used as an extractive summary of the social media stream at a certain time. This does not take into account pr
We also propose a quality ranking scheme that makes the mining results presentable to human users. We don't call this a summary because we don't take into account the quality measures of summaries such as coherence and cohesion. However, the top ranked itemsets cover a variety of open topics, and within one topic different opinions are reported as separate contrastive itemsets.  
% TODO: By the date of the camera ready version, please have a url to put and tell people to go to and look! However,

The rest of the paper is organized as follows: We start by an overview of properties of text in social media and describe the dataset we use in section \ref{sec:socmed}. Then we explain the Frequent Itemset Mining algorithm which we build upon in section \ref{sec:fim}. 
In the next 3 sections we describe the adaptations we propose for making the algorithm suitable for mining social media text. In section \ref{sec:ngrams} we describe how using term N-Grams of variable lengths filters out many language constructs. In section \ref{sec:KLD} we describe the use of differential temporal features to rank interesting itemsets. In section \ref{sec:strong} we propose the strong closed property and show how to reduce the number of itemsets without loosing important ones. After that we show an example of the results of applying our methods in section \ref{sec:emperical}. In section \ref{sec:related} we discuss related work that applied Frequent Itemsets Mining on text. Finally, we finish by the conclusion and future work in section \ref{sec:concfut}.

%Mining user generated content has been a hot topic since the beginning of Web 2.0. \textbf{didn't I say I dislike such inaugrational sentences.. ehem!} Businesses are interested in getting direct insights from consumers without incurring costs for surveys, and so are governments, celebrities, and above all advertisers and market analysts. Researchers are interested because methods developed for other media does not necessarily work for social media. This is particularly correct for Twitter because of its length restriction, high volume and lack of structure. The length restriction has resulted in a very particular language use as detailed in \ref{sec:socmed}, but it seems to have also played a dual role in encouraging people to keep Tweeting about everything they do, think or feel. This resulted in a very high volume of Tweets\footnote{We receive an average of 100,000 Tweets per hour from Spritzer}, most of which is personal updates but some of which represent what is happening now. The speed of news on Twitter has been known to exceed the news on professional news agencies \cite{firststory}, and it even travels faster than earthquakes \cite{earthquake}. First stories that appear in Twitter rather than official news are covered by Twitter user present at the event, and able to provide content that cannot be acquired after the fact such as the picture of the emergency landing on the Hudson river \cite{hudson}. Such stories are noticed because of the natural collaborative filtering act of retweeting good content, and the use of the community developed convention of Hashtags. The Twitter website makes use of this by following the rise in the volume of keywords, and featuring keywords whose rise in volume accelerates more than any other. However, this does not catch topics with sustained interest. It also fails to catch important topics for which the interest is peaking within a certain niche of users. Further it is almost always polluted by uninteresting topics that are made trending. In this paper we propose...

\begin{figure*}
\centering
% Generated with LaTeXDraw 2.0.8
% Thu May 02 21:16:47 EDT 2013
% \usepackage[usenames,dvipsnames]{pstricks}
% \usepackage{epsfig}
% \usepackage{pst-grad} % For gradients
% \usepackage{pst-plot} % For axes
\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-4.41)(17.873497,4.4013004)
\pstriangle[linewidth=0.04,dimen=outer](8.57,-4.41)(17.14,8.82)
\psline[linewidth=0.04cm](0.78,-3.59)(16.32,-3.61)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](6.88,2.57)(10.24,2.59)
\usefont{T1}{ptm}{m}{n}
\rput(8.465957,-4.005){Items with low frequency; excluded by Frequent Itemset Mining since they are below the support threshold}
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](2.44,-2.03)(14.78,-2.01)
\usefont{T1}{ptm}{m}{n}
\rput(4.0392385,4.075){Items with the highest frequencies within transactions }
\usefont{T1}{ptm}{m}{n}
\rput(3.7760057,3.675){sharing a common prefix are most likely language }
\usefont{T1}{ptm}{m}{n}
\rput(3.4375684,3.295){constructs, but they can also be topical words}
\usefont{T1}{ptm}{m}{n}
\rput(13.305957,4.075){In section 3 we reduce the number of itemsets produced}
\usefont{T1}{ptm}{m}{n}
\rput(12.990498,3.675){through flattening the peakedness of this head.}
\usefont{T1}{ptm}{m}{n}
\rput(2.1082716,0.675){The closed property selects }
\usefont{T1}{ptm}{m}{n}
\rput(1.8648047,0.295){non-redundant itemsets, }
\usefont{T1}{ptm}{m}{n}
\rput(3.0692382,2.295){Itemsets whose lowest frequency item is}
\usefont{T1}{ptm}{m}{n}
\rput(2.732578,1.895){within the mid-range of frequencies}
\usefont{T1}{ptm}{m}{n}
\rput(2.409707,1.515){contain interesting information,}
\usefont{T1}{ptm}{m}{n}
\rput(2.4226954,1.095){but there are too many of them.}
\usefont{T1}{ptm}{m}{n}
\rput(1.8226953,-0.105){but it is easily violated.}
\usefont{T1}{ptm}{m}{n}
\rput(14.025957,2.295){In section 4 we propose the strong closed}
\usefont{T1}{ptm}{m}{n}
\rput(14.607364,1.895){property. It selects only itemsets comprizing}
\usefont{T1}{ptm}{m}{n}
\rput(14.774668,1.495){a high proportion of the support of their}
\usefont{T1}{ptm}{m}{n}
\rput(14.486103,1.075){closed subsets, after combining...}
\usefont{T1}{ptm}{m}{n}
\rput(14.701895,0.695){For redundancy because of the}
\usefont{T1}{ptm}{m}{n}
\rput(14.806572,0.295){use of language, clustering}
\usefont{T1}{ptm}{m}{n}
\rput(15.167656,-0.105){of the remaining itemsets}
\usefont{T1}{ptm}{m}{n}
\rput(15.264932,-0.525){solves the problem, as}
\usefont{T1}{ptm}{m}{n}
\rput(1.5603125,-0.505){It also doesn't solve}
\usefont{T1}{ptm}{m}{n}
\rput(1.3780762,-0.905){redundancy from }
\usefont{T1}{ptm}{m}{n}
\rput(1.2769824,-1.705){in language.}
\usefont{T1}{ptm}{m}{n}
\rput(1.2849317,-1.305){slight variations}
\usefont{T1}{ptm}{m}{n}
\rput(15.70628,-0.925){discussed in section 8.}
\usefont{T1}{ptm}{m}{n}
\rput(8.459648,-2.345){After mining itemsets based on support and reducing their redundancy, we rank}
\usefont{T1}{ptm}{m}{n}
\rput(8.603467,-3.265){specially in text where frequency has a Zipfean distribution. Section 9 describes our ranking scheme.}
\usefont{T1}{ptm}{m}{n}
\rput(8.574101,-2.805){the remiaining itemsets using temporal features. The majority of itemsets has low support, }
\usefont{T1}{pcr}{m}{n}
\rput(8.502881,3.535){the}
\usefont{T1}{pcr}{m}{n}
\rput(7.644902,2.835){obama}
\usefont{T1}{pcr}{m}{n}
\rput(8.208155,1.035){won}
\usefont{T1}{pcr}{m}{n}
\rput(7.4023533,1.875){president}
\usefont{T1}{pcr}{m}{n}
\rput(10.234902,-1.625){\#mtvema}
\usefont{T1}{pcr}{m}{n}
\rput(9.792881,2.175){bieber}
\usefont{T1}{pcr}{m}{n}
\rput(10.138154,1.475){justin}
\usefont{T1}{pcr}{m}{n}
\rput(11.074903,0.655){selena}
\usefont{T1}{pcr}{m}{n}
\rput(7.9116306,0.115){elections}
\usefont{T1}{pcr}{m}{n}
\rput(7.711367,-0.925){\#elections2012}
\usefont{T1}{pcr}{m}{n}
\rput(11.507587,0.055){gomez}
\usefont{T1}{pcr}{m}{n}
\rput(5.996133,0.615){barack}
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(8.56,4.35)(7.78,3.05)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.68,2.65)(7.32,2.03)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(6.84,1.63)(6.4,0.81)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(8.38,0.83)(8.34,0.37)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(8.26,-0.11)(8.28,-0.69)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(8.62,2.59)(8.42,1.25)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(8.56,4.29)(8.5,3.71)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(8.62,4.35)(9.82,2.41)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(9.18,0.85)(10.0,-1.39)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(11.14,0.43)(11.3,0.19)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(6.04,0.43)(5.4,-1.03)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(9.96,1.97)(10.22,1.61)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(9.44,2.01)(9.12,1.21)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(10.42,1.29)(10.74,0.83)
\usefont{T1}{pcr}{m}{n}
\rput(8.844902,2.815){obama}
\usefont{T1}{pcr}{m}{n}
\rput(9.168155,1.055){won}
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(8.54,3.29)(8.56,2.93)
\usefont{T1}{pcr}{m}{n}
\rput(5.4113674,-1.365){\#elections2012}
\end{pspicture} 
}
\caption{Roadmap of the paper overlaid on an frequency ordered prefix tree}
%, where the higher the item in the tree the more its frequency is. The}
\end{figure*}

\section{Related Work}
\label{sec:related}
Frequent Itemset Mining is a very large body of work that goes back to the early 90s. We cover the topic only briefly, as the focus of this paper is not Frequent Itemsets Mining but rather its adaptation to social media text. The original Apriori algorithm \cite{agrawal1994fast} and algorithms based on it suffer performance degradation and a big increase in memory requirement when the number of distinct items is high. This is caused by the candidate generation bottleneck as explained later. Another famous class of mining algorithms is the FP-Growth \cite{han2000mining} based algorithms. FP-Growth  skips the candidate generation step, and instead creates a succinct representation of the data as a frequency ordered prefix tree called the FP-tree. An FP-tree imposes the invariant that within a branch the frequency is non-increasing from the root down to the leaves. 
%While this class of algorithms performs better than the Apriori based ones,
The memory requirements of FP-Growth algorithm suffers from the sparsity of the data, since the data structure is succinct only if it can find common prefixes within the constraints of its invariant. 
%Also, it is not suitable for 
A less known algorithm that is robust against data sparsity is LCM \cite{uno2004lcm}, which we will describe in detail in section \ref{sec:lcm}. We use the implementation submitted to the workshop for Frequent Itemset Mining Implementations (FIMI) '04 \cite{DBLP:conf/fimi/2004}, which is the workshop's award winner. In section \ref{sec:runtime} we show that our extensions do not degrade the performance of LCM by comparing the extended LCM to the runner up at FIMI '04 \cite{grahne2004reducing}. 
%The extended LCM still beats the runner up
%and we found to perform very well on the Twitter dataset we have is LCM \cite{uno2004lcm}

%Frequent Itemset Mining was applied to all kinds of data, including social media text. In \cite{yang2012framework}
%, which we found out about only recently, 
The problem that there are too many itemsets, with a lot of noise and redundancy, is addressed either by clustering \cite{yan2005summarizing} the itemsets or by mining only itemsets that satisfy a certain condition. The closure condition \cite{pasquier1999discovering} is a prominent condition upon which many other conditions are based. Most similar to the strongly closed itemsets we  propose are the $\delta$-covered \cite{xin2005mining} and the $\delta$-free \cite{boulicaut2003free} sets. The $\delta$-covered condition is exactly the opposite of the strong closure condition, and it ``relaxes the closure condition to further reduce pattern set size'' \cite{liu2012finding}. The $\delta$-free condition is like the strongly closed condition but it is used to eliminate different itemsets, since the motivation is to provide a compressed representation of itemsets by sacrificing support information. 
%In fact, t

The motivation of most of the work in finding representative itemsets is compressing itemsets mined from a general dataset. This leads to decisions different than what  are made in order to find the most informative itemsets mined from text.
For example, in \cite{li2010mining} itemsets are mined from paragraphs of newswire text, and used to determine term weights for query expansion. Improvements in performance have been achieved by using itemsets known to come from a training set of related documents as well as ones from unrelated documents. In \cite{laumicroblog}, similar methods of term weighting were used in pseudo-relevance feedback  for Twitter search, and achieved improvements on a weak baseline. \textbf{should I include the afore mentioned attempt?}
On the other hand, in %a recent work 
\cite{yang2012framework} LCM is also used to mine Twitter posts and then a few itemsets are selected as a ``summary'' of the data, but they are selected according to their utility in a
%, similar to our work, the authors make an effort to select only a few frequent itemsets. However, they address the problem as a stream compression problem and they select itemsets based on utility in their proposed
lossy compression algorithm.
%, SPUR. SPUR is based on representing transactions as itemsets that cover them, and it might introduce false positives and negatives. The algorithm could meet the desired  constraints on memory consumption (set at 20MB) and on processing data as a stream, but there was no justification why such tight constraints should be set. On the other hand, 
The quality of the summary seems to be affected by the choice of utility function, but the only assessment made about this was showing that the actual transactions can be reconstructed from the summary with an accuracy that is expected to be high if the Tweet contains ``both recent and frequent keywords''. In the experiments, non-negative matrix factorization is used to extract topics from parts of the summary matching a query (world cup) and specific time intervals (before certain matches). It is unclear whether the raw summary would cover a variety of topics, specially non-trending ones, and no ranking scheme was shown to pick interesting topics without specifying a query. Moreover, the data is preprocessed by stemming and stop words removal, which should require a language identification component upstream but it is not discussed.
%the algorithm is used to summarize Tweets matching the query ``world cup'' from specific time intervals (during the world cup), then nonMatrix Factoization but the top chosen patterns are not compelling. 

%There are two other works from the same author, but their results are not good.. should I cite? Pattern Cooccurrence Matrix and Queensland in TREC Microblog 2011



\section{Frequent Itemset Mining}
\label{sec:fim}
\subsection{Preliminaries}
Traditionally, Frequent Itemset Mining is applied to a \emph{database} of \emph{transactions} made at a retail store. This terminology is suitable for market basket data and we will stick to it out of convention, even though we are mining text where the terms corpus and document are normally used. Because of the dynamic nature of social media, rather than giving the whole database as input to mining algorithms, the input is an \emph{epoch} of data; data with timestamps within a certain period of time. The epoch's \emph{span} is the length of this period in hours, and the \emph{volume} at this epoch is the number of transactions in the epoch divided by its \emph{span}.

A \emph{frequent itemset} is a set of items that occur together a number of times higher than a given threshold, called the \emph{support} threshold. We also adapt the support threshold to the dynamicity of the \emph{volume} at different times of the day. We define the \emph{minimum support threshold} as the threshold at the hour of the least \emph{volume} during the day.  The \emph{minimum support} is supplied as an absolute number, $a$, and then converted to a ratio, $\alpha = \frac{a}{avg(volume_{daily-minimum})}$.
%\begin{equation}\alpha = \frac{a}{volume_{daily-minimum}}\end{equation}
%The actual support threshold used for mining any given \emph{span} of time is thus $\alpha$ multiplied by the number of transactions in the \emph{epoch}. 
The actual support threshold used for mining any given epoch is thus $\alpha$ multiplied by the \emph{epoch}'s \emph{volume}. 

%frequently; for example, when mining Tweets posted on November 6th, 2012 using terms as items, the set \{of, president, states, the, united\} is a frequent itemset (ordered lexicographically). Traditionally, Frequent Itemset Mining is applied to a \emph{database} of \emph{transactions} made at a retail store. This terminology is suitable for market basket data, but as we are mining social media text we use the term \emph{document} instead of transaction. It is noteworthy that this term is used as a matter of convention, yet our methods are tailored for the short ``documents'' typical of social media as described earlier. Because of the dynamic nature of social media, we call the input to a mining algorithm an \emph{epoch} of data rather than a database. An epoch of data is all documents posted in a certain period of time, the length of this period is the epoch's \emph{span}.
% in a dynamic "corpus" typical of social media as described earlier.\emph{corpus} instead of database

%The \emph{support} of an itemset is the number of times it appears, as a ratio of the number of documents in the epoch. The \emph{minimum support} is a threshold separating \emph{frequent} itemsets from \emph{infrequent} ones. Even though selecting itemsets based on their frequency of appearance was criticized in the domain of market basket data because it generates obvious itemsets, it makes sense in the domain of social media because this acts precisely as a collaborative filter. 

We now introduce the notation used in this paper:

\begin{itemize}
\item $I = \{i_1,i_2,..,i_n\}$: The set of all items (vocabulary).
\item $t_a = \{i_{a1},i_{a2},..\}$: A transaction made up of as a set of items, not necessarily terms. Each transaction has a sequential id denoted by the subscript letter.
\item $E^{span} = \langle t_a, t_b, ..\rangle$: An epoch of data of a certain span, such as an hour, made up of a sequence of transactions.
\item $s \subset I$: An itemset, its support is given by $|T_s|$.
\item $T_s = \{t: s \subseteq t\}$: All transactions containing itemset s.
\item $|.|$: Cardinality operator; gives the size of the operand.
%\item $\|.\| = \sum_{x \in .}{|x|}$: The Manhattan norm.

\end{itemize}

\subsection{Background}

The two basic operations of Frequent Itemset Mining algorithms are \emph{Candidate Generation} and \emph{Solution Pruning}. The original Apriori algorithm by Agrawal et al. \cite{agrawal1994fast} generates candidates of length \emph{K} (\emph{K}-itemsets) by merging frequent itemsets of length \emph{(K-1)} (\emph{(K-1)}-itemsets) that differ in only 1 item, usually the last item given a certain total ordering of items. By using only frequent  \emph{(K-1)}-itemsets for generating candidate \emph{K}-itemsets a lot of possible \emph{K}-itemsets are implicitly pruned, based on that all subsets of a frequent itemset has to be frequent (the Apriori property). This still generates a very large number of candidates, specially in early iterations of the algorithm. Consider, for example, the generation of candidate \emph{2}-itemsets from a database. This requires producing all unordered pairs of \emph{1}-itemsets (terms), after pruning out rare ones with frequency less than the support threshold. In many domains, including text mining, the number of fequent \emph{1}-itemsets is large enough to prohibit generating a number of candidates in the order of this number squared. In text mining, a rather low support threshold has to be used, because the frequency of terms follow a long tailed Zipfean distribution.

\section{Basic Algorithm}
To overcome the bottleneck of \emph{Candidate Generation}, many algorithms are proposed to take hints from the transaction space rather than operating blindly in the item space, each based on a certain property that helps pruning out more candidates. In this paper we expand on LCM \cite{lcm}, an algorithm based on a property of a certain class of itemsets called \emph{Closed Itemsets}. A formal definition of closed itemsets is given in equation \ref{eq:Closed}: 

\begin{equation}\label{eq:Closed}\mathcal{C} = \{S_c:\, \nexists \, S_d \, where \, S_c  \subset S_d \, and \, \|D_{S_c}\| = \|D_{S_d}\|\}\end{equation}

The properties of Closed Itemsets are the following:
\begin{enumerate}
\item An itemset is closed if adding any item to it will reduce its support. 
\item A subset of a closed itemset is not necessarily closed, but one or more closed subset must exist for any itemset (formally this could be the empty set, given that any item that appears in all transactions is removed in a preprocessing step). 
\item If a closed \emph{K}-itemset can be extended any further then one of its supersets will be closed, however not necessarily a \emph{(K+1)} superset. Itemsets that cannot be extended any further are called \emph{Maximal Itemsets}, and they are a subclass of closed itemsets.
\end{enumerate}

Besides being much smaller than the solution space of frequent itemsets, the solution space of closed itemsets can be navigated efficiently. By using an arbitrary total ordering of items, any closed itemset can be considered an extension of exactly one of its subsets. Thus, only this subset is extended during candidate generation. All the other subsets do not need to be extended by items that would lead to the longer closed itemset.  This is called \emph{Prefix Preserving Closure Extension (PPC-Extension)} and it is proposed and formally proved in \cite{lcm}. This is achieved by following three rules, which we state after a few definitions to facilitate their statement. First, an item is \emph{larger/smaller} than another item if it comes later/earlier in the total ordering. This terminology comes from that LCM is most efficient if the items are ordered in ascending order of their frequency. Second, the \emph{suffix} of an itemset is one or more items whose removal does not result in an itemset with higher support. Notice that they will necessarily be at the end of the itemset, regardless of the total ordering. Finally, we call the first item added to the suffix of the itemset its \emph{suffix head}. With this terminology, the rules for \emph{PPC-Extentsion} are:
\begin{enumerate}
%\item The total ordering is used to generate \emph{1}-itemsets 
\item An itemset can be extend only by items \emph{larger} than its \emph{suffix head}. Extending by \emph{smaller} items will lead to closed itemsets already generated.
\item After forming an itemset $S$, add to its \emph{suffix} all items whose frequency within $D_S$ is equal to $\|D_S\|$. %Maintain the order of addition within the suffix.
%\item If the order of the suffix does not follow the total ordering, prune this solution branch. 
\item If any item in the \emph{suffix} is \emph{smaller} than the suffix head, prune this solution branch. All closed itemsets within this branch have already been generated.
%\item When extending by an item that has \emph{friends}, within the context of the current itemset, all \emph{friends} are added since they appear only together.  
%\item Skip any extension item that has a \emph{friend} item \emph{larger} than it. This rule can be reversed.
%, possibly affecting performance.
%, depending on what would result in smaller merges. 
%The solutions generated by adding this \emph{smaller} item to the itemset will be generated when adding the \emph{larger} item to the itemset. %, and thus it will be redundant to 
\end{enumerate}
 

%Second, an item is a \emph{friend} of another item in a certain context if they both appear only together in this context. A context is a subset of the database, for example the transactions that contain a certain item. Finally, the smallest item in the friends of the last item of the itemset is called the \emph{suffix head} of the itemset.

%Finally, the  maximum item whose removal does not change the support of an itemset is called the \emph{core item} of the itemset. It is usually the pre-last item, but not necessarily so if the last item has friends. 
% (which is equivalent to the item's postings list in an inverted index). 

Table \ref{table:PPCExample} is an example of how \emph{PPC-Extentsion} is used to generate closed itemsets starting from the  \emph{1}-itemset `barack'. The upper table enumerates $D_{barack}$. %, the documents containing `barack'. 
The lower table shows steps of itemsets generation. The current solution along with its frequency is in column 2, solutions marked by an * are the closed itemsets emitted. All possible extension items and their frequencies are in column 3 with the one being considered bolded. Column 4 is a comment explaining the step. At each step, a pass is done on $D_{itemset}$ to enumerate and count possible extension items.
% After the counting step, the rules of PPC-Extension are followed to decide whether to output an itemset as a closed one or not. 
To enforce a support threshold infrequent extension items are removed, but in this example there isn't such a threshold. Notice that the number of steps is linear in the number of closed itemsets, and the only additional storage required besides the storage of the documents is that of the possible extension items. Of course this is a simplified example, but it shows in essence how LCM achieves its low run time and memory requirements. We refer the interested reader to \cite{lcm} for a theoretical proof that the algorithm runs in linear time in the number of closed itemsets, and that this number is quadratic in the number of transactions. Performance on a real data set is shown in section \ref{sec:socmine}. We proceed by describing how to implement this algorithm using an inverted index.

%all closed itemsets were generated in 5 steps (not counting steps shown only for explanation), with only one redundant step  
%The most important contribution of LCM is that it generates closed itemsets from 

\begin{table*}
\centering
\begin{tabular}{|c|p{5cm}||c|p{5cm}|} \hline
Doc. Id & Document & Doc. Id & Document\\\hline
a& barack \& mitt & b & brack obama \& mitt romney  \\\hline
c& brack obama \& romney & d & brack obama  \\\hline
\end{tabular}
\begin{tabular}{c}
Documents (two per row)\\\\
\end{tabular}
\begin{tabular}{|c|p{4.5cm}|p{5cm}|p{6cm}|} \hline
Step&Current Solution&Possible Extension Items&Comments\\ \hline
1& \{barack\} (4)* & \textbf{mitt} (2), obama (3), romney (2) & Items are ordered lexicographically\\ \hline
2& \{barack,mitt\} (2)* & \textbf{obama} (1), romney (1) & Extension items reenumerated \& counted\\ \hline
%`obama' and `romney' are \emph{friend} items in this context. \\ \hline
%3 & \{barack, mitt\} (2) & \sout{obama} $\rightarrow$ \textbf{romney} & Rule 3: `obama' skipped. If ordered by \\\hline
3 & \{barack,mitt,obama\} (1) & romney (1)                       & Rule 2: `romney' appears in all $D_{itemset}$\\\hline
%freq. of `romney' = $\|D_{itemset}\|$ \\\hline
4 & \{barack,mitt,obama,romney\}(1)* & & Rule 2: `obama'  is the \emph{suffex head} \\\hline
5 & \{barack\} (4) & mitt (2), \textbf{obama} (3), romney (2) & Nothing more to add, back to `barack' \\\hline
%, extension item `obama'. \\\hline
6 & \{barack, obama\} (3)* & \sout{mitt} (1), \textbf{romney} (2) & Rule 1: skipping `mitt', adding `romney' \\\hline
7 & \{barack, obama, romney\} (2)* & \sout{mitt} (1) & Rule 1: Nothing more to add. \\\hline
8 & \{barack\} (4) & mitt (2), obama (3), \textbf{romney} (2) & Back to `barack', adding `romney' \\\hline
9 & \{barack, romney\} (2) &  \sout{mitt} (1), obama (2) & Rule 2: add obama to suffix after `romney' \\\hline
10 & \{barack, romney, obama\} (2) &  \sout{mitt} (1)  & Rule 3: suffix isn't ordered, prune solution\\\hline

\end{tabular}
\begin{tabular}{c}
Closed itemsets containing `barack'
\end{tabular}
\caption{Generation of closed itemsets by Prefix Preserving Closure Extension}
\label{table:PPCExample}
\end{table*}

\subsection{Implementation details}
We show in algorithm \ref{algo:lcmix} how to implement LCM and PPO-Extension using an inverted index. The algorithm takes as input an epoch of data and a support threshold as a ratio $\alpha$. It outputs the closed itemsets with support more than the threshold. Along with each itemset in the solution, it also outputs the transactions in which it occurs - which is represented as $\langle items, \|D_{itemset}\| \rangle$. The symbol $\succ$ denotes that the lefthand side  succeeds the righthand side in the total ordering.

The algorithm also lends itself to distributed implementations easily. For example, a Map/Reduce implementation is easy since the only operations are counting (line 14) and projection (line 22). However, the fast execution time and the low memory requirements of the algorithm makes it possible that a distributed implementation will cause an overhead. In the implementation shown, it is not necessary that the index's tokens list follow the total ordering; all itemsets of length 1 will be considered anyway. 
%The first line assumes that any item present in all documents is removed in a preprocessing step
%By tracing the call tree of the recursive implementation shown, we 

\begin{algorithm}
\SetAlgoLined
\LinesNumbered
\SetKwProg{Fn}{Function}{ is}{end}
\KwIn{$\alpha$: Dynamic support ratio}

\KwData{E: Epoch of data}
\KwResult{C: Closed itemsets having support $\alpha$ within E}
C $\gets \{\langle \emptyset, E\rangle\}$ \tcp*{$\emptyset$ is a closed itemset!}
X $\gets$ Inverted index of E\;
%(\tcp{follow total ordering})
\ForEach{$i \in X.tokens$}{
	$D_{\{i\}} \gets$ X.postingsList[$i$]\;
	\lIf{$\|D_{\{i\}}\| \geq \alpha \frac{\|E\|}{E.span}$}{
		LCM($\{i\}, i, D_{\{i\}}$)
	}
}
\Return{C}\;

\Fn{LCM(S: Current itemset,   $i_{sh}$: Suffix head, \\ $D_S$: Documents (transactions) containing S)}{
	frequency[$1 \ldots i_n$] $\gets$ 0\;
	suffix $\gets \{ i_{sh}\}$\;
	\ForEach{$d \in D_S$}{
%(\tcp*{Documents can be stored according to total ordering})
		\ForEach{$i \in d$}{ % such that $i \succ i_{sh}$}{
			frequency[$i$]++\;
			\lIf{frequency[$i$] $ = \|D_S\|$}{
				suffix.add($i$)	
			}
		}
	}
	\lIf{$\exists j: i_{sh} \succ suffix[j]$ }{
		\Return
	}
	C.add($\langle S \cup suffix, D_S\rangle$)\;
	\ForEach{$i \succ i_{sh}$ and $i \notin $ suffix}{
		\If{frequency[$i$] $\geq \alpha \frac{\|E\|}{E.span}$}{
			$D \gets D_S \cap i$	\tcp*{Results of query $S$ AND $i$}
			LCM($S \cup suffix \cup \{i\}, i, D$) 
		}
	}
}

\caption{LCM Frequent Itemsets Mining}
\label{algo:lcmix}
\end{algorithm}

%The algorithm described works well on the sparse data typical to text mining. 
%FIMI??
%In the next few sections we will see how to make the efficient fre also effective.

\section{Mining social media}
\label{sec:socmine}

Throughout this paper we use data we have been collecting from the Twitter public stream\footnote{https://dev.twitter.com/docs/streaming-apis/streams/public} as of October 1st, 2012. 
% at 00:00 HST. We use Hawaii Standard Time as our reference 
We use only Tweets written in Latin script to facilitate tokenization using white space and other word boundaries. We collected only the Tweet text to avoid reliance on any features specific to a certain social medium, and make the algorithms applicable to other media where text is short such as comments and Facebook or Google+ status updates. The only preprocessing performed was removing duplicate original Tweets (not retweets) using a Bloom filter. This removes spam Tweets sent by BotNets, such as advertisements about Raspberry Ketone diets.

We apply the algorithms to epochs of data, so they are not strictly stream processing algorithms. However, we regard the process as mining a sliding window that is moved forward by time steps of short span. The time step must be longer than the time needed to mine an epoch of data, and the performance of our algorithms makes it possible to use a time step of a few seconds for epochs up to a day long. Figure \ref{fig:runtimeEpochs} shows the runtime of LCM on epochs of increasing length, and we will show in section \ref{sec:} that our extensions don't degrade the performance. The times reported in figure \ref{fig:runtimeEpochs} are averages across all epochs of the specified length in the months of October, November and December, using a time step that is half the epoch length. The variance is very low and the confidence bands are not shown because they appear as dots on top of the bars. 
 %much shorter than time needed for human perception. 
% arbitrary span is processed in much less time than its span.

The support threshold used throughout this paper, unless otherwise specified, is $\alpha=0.0002$. This is determined as follows: We picked a topical term that is known to  steadily  appear with a rather high frequency, and is talked about in all languages; `obama'. The maximum likely hood estimate of the probability of the term `obama' within the whole collection of Tweets is 0.0001. Since the average number of Tweets per hour is 100000, so the term `obama' is expected to appear 10 times per hour on average. Thus, we use a minimum support threshold of 10, which translates into $\alpha = 0.0002$.

In the rest of this paper we mine epochs of 1 hour span. The reason behind this choice is an observation that the number of closed itemsets mined from epochs of span 1 hour or more, at the same support threshold, remains that same. This indicates that itemsets mined from shorter epochs of social media text are not included in the results of mining longer epochs. 
%We will show that itemsets mined from epoch of span 1 hour are not only languag
Therefore, the epoch span should be minimized. However, when the epoch span is shorter than an hour the frequency required to surpass the support threshold becomes very low, and number of mined itemsets increases because a lot of noise itemsets are mined.

\begin{figure}
\centering
% Generated with LaTeXDraw 2.0.8
% Wed May 01 10:13:00 EDT 2013
% \usepackage[usenames,dvipsnames]{pstricks}
% \usepackage{epsfig}
% \usepackage{pst-grad} % For gradients
% \usepackage{pst-plot} % For axes
\scalebox{.4} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-5.61)(19.94,5.61)
\rput(9.97,0.0){\includegraphics{runtime_different-epoch-spans_bar-table.eps}}
\rput(9.8,1.44){\includegraphics{runtime_different-epoch-spans_line.eps}}
\usefont{T1}{ptm}{m}{n}
\rput{90.15449}(0.96138316,-0.45039606){\rput(0.6871191,0.225){\LARGE Time in milliseconds}}
\usefont{T1}{ptm}{m}{n}
\rput(10.130039,-0.925){\Large Epoch span in hours}
\end{pspicture} 
}
\caption{Mean runtime at different epoch spans}
\label{fig:runtimeEpochs}
\end{figure}

\subsection{Mining term N-Grams}
\label{sec:ngrams}

A large number of itemsets are language constructs that bear no information, such as ``such as''.  By treating sequential language constructs, and any other multiword expression, as one item we eliminate a large number of such itemsets. Actually, we also eliminate itemsets that are made up of all the different fragments of the language construct along with other items; for example, \{we, did, it, \#teamobama\} can produce 10 other combination of length 2 or more. There are many measures of association that can be used to detect multiword expressions, but each measure is good only under certain conditions and has special properties \cite{ramisch2012broad}. We experimented with various measures, and in fact we found out that a very good measure for identifying multiword Named Entities is Yule's Q; a measure of association and disassociation derived from the odds ratio. However, we have finally found that for the purpose of preprocessing before Frequent Itemsets Mining what works best is tokenizing the documents into term N-Grams with varying N. 

We start by tokenizing into unigrams, and counting their frequencies. Then we use the probability of the same word used to determine the support threshold, $P(`obama') = 0.0001$, and assume that this is where the head of the Zipfean distribution starts. For each unigram belonging to the head, we create two term bigrams by attaching to it the unigrams before and after it. We repeat this foreach N $\ge$ 1 by creating two (N+1)-Grams for all N-Grams with probabilities above the threshold until there are no more such N-Grams. We do not prevent overlap, because there is no guarantee that the N-Gram created makes any sense. At each N, the probability threshold is adjusted to account for the increase in the number of tokens and the overall increase in the grand sum of counts, since each high frequency N-Gram is replaced by two lower frequency ones. Let the original probability threshold be $\eta$, then the adjusted $\eta_N$ is:
\begin{equation}\eta_N = \eta * \frac{\sum_{\{i:\, i \,\in\, I\, and\, i.length \,\le\, N\}}{freq(i)}}{\sum_{\{i:\, i\, \in\, I \,and \,i.length\,=\,1\}}{freq(i)}}\end{equation}
% At each N, the probability threshold is adjusted by the ratio of the sum of the counts of all terms of length up to (N+1), to the the sum of the counts of all terms of length up to N-Gram.

%as described above 
%Throughout the discussion above, 
%In the next few sections we will look at ways to reduce the number of itemsets produced and filter out uninteresting ones. 
%counts the support of itemsets 
%While extremely fast, Frequent Itemsets Mining produces numerous itemsets that are not interesting. Actually it was originally proposed as a preliminary stage to Association Rules Mining, which sifts through the relatively high number of itemsets and produces a smaller number of association rules. The first type of uninteresting itemsets is the language constructs that bear no information, such as ``such as'', and itemsets that are made up of all the different fragments of the language construct along with other items. The latter type is an artifact of itemset mining when the final itemset has such a language construct within it; for example, \{we, did, it, \#teamobama\}.
% One possibility to get rid of such itemsets is to mine itemsets 
% and might produce wrong results otherwise
%If we can treat sequential language constructs, and really any other multiword expression, as one item we reduce the number of itemsets produced in these two ways. %just described.
%itemsets that are parts of it will not be produced, and fragments of it along with other itemsets in the rest of 
%they will not be produced as itemsets (since we produce only itemset of length 2 or more). 



%We start by tokenizing into unigrams, then we
%producing term N-Grams from the window surrounding any token 
%create two term bigrams for each unigram appearing with a probability above a certain threshold, $\eta$, by attaching to it the unigrams before and after it. We do this recursively by increasing N and creating two (N+1)-Grams for each N-Gram above the threshold until there are no more N-Grams above the threshold. 
%At each value of N, the threshold $\eta$ is adjusted by the ratio of the number of tokens of length up to N and the number of unigrams; $\eta_{N} = \|\eta_1\| * \frac{\| I_{N}\|}{\|I_{1}\|}$.
% thus flattening the Zipfian distribution a little. 
%Following we present an example of tokenizing ``Barack Obama president of the United States'':

%\begin{enumerate}
%\item First the tokens will be: \texttt{`barack', `obama', `president', `of', `the', `united', `states'}. 
%\item Assume that after tokenizing the whole epoch we find that the probability of \texttt{`obama', `of'} and \texttt{`the'} within the current epoch is more than $\eta$. Therefore the tokens are changed to: \texttt{`barack',  `barack + obama', `obama + president', `president', `president + \\of', `of + the', `the + united', `united', `states'}.
%\item Assume that the bigrams  \texttt{`barack + obama'} and \texttt{`of + the'} still appear with a probability higher than %the adjusted
%$\eta$. Therefore trigrams are generated by attaching the unigrams before and after them, and the tokens become: \texttt{`barack',  `barack + obama + president', `obama + president', `president', `president + of', `president + of + the', `of + the + united', `the + \\united', `united', `states'}
%\item At N=3, we find that no trigrams still appear with probability greater than $\eta$, and thus terminate.
%\end{enumerate}

%Note that in the above procedure we didn't prevent overlap, because there is no guarantee that the N-Gram created makes any sense. The goal is to create a single unit out of frequent sequences and this is achieved because an N-Gram keeps growing as long as it is frequent.
%, which will stop as soon as a unigram that is not part of the expression is attached to it. We can look at this as flattening the Zipfian distribution by creating more tokens with less frequency each, but the distribution of the input doesn't affect the results of itemsets mining - only its runtime can be affected.

Figure \ref{fig:ngramsLen} shows the effect of increasing the maximum length of N-Grams from 1 to 5 on the number of tokens, the number of closed itemsets of length more than 1, and the runtime of mining 1 hour epochs of data. The values shown are averages across all 1 hour epochs in the month of November. The value of $\eta$ used is 0.0001.
%The value of $\eta$ used is 0.0001, which is the probability of the term `obama' within the whole collection of Tweets. 
%The support threshold used for mining in this experiment and all other experiments is also derived from the probability of the term `obama', since it is a known to be steadily frequent and talked about in all languages. The average number of Tweets per hour is 100000, so the term `obama' is expected to appear 10 times per hour on average. We use a minimum support threshold of 10, which translates into $\alpha = 0.0002$.
Figure \ref{fig:ngramsLen}(a) shows that the number of distinct items increases a lot when N moves from 1 to 2, then keeps increasing slightly until it starts decreasing at N=5. The decrease happens because all 4-Grams with probability above the threshold are parts of Tweets from services that use the same text and append a URL, such as Tweets reporting scores from Game Insight\footnote{http://www.game-insight.com/}. Such Tweets are tokenized into more 4-Grams than 5-Gram, and the 4-Grams appearing in them don't appear elsewhere; thus each two of them are reduced into one 5-Gram.  Figure \ref{fig:ngramsLen}(b) shows that the number of itemsets keeps decreasing as expected.  Figure \ref{fig:ngramsLen}(c) shows that runtime also decreases as N goes from 1 to 5, since LCM runtime is proportional to the number of closed itemsets, and is not affected by the sparsity of data. The runtimes in this figure are slightly less from those in figure \ref{fig:runtimeEpochs} because they don't include the time taken for writing the posting lists.
% robust against  the increasing
%At N=5 there is a slight increase of 44 milliseconds in the average runtime. We did not try to justify the increase and we suspect that it is caused by a slight difference in the condition of the shared machine at the time of running the experiment with N=5.
%yet statistically significant increase. We suspect that this increase is due to the  

%\begin{figure}*
%\begin{center}$
%\centering
%\begin{subfigure}{.3\textwidth}
 % \centering
% \begin{array}{ccc}
%\includegraphics{perf_mean-distinct-items-changing-ngram_n1-5_s10+_1hr.png}
%\caption{(A) Mean number of distinct items}
%\end{subfigure}%
%\begin{subfigure}{.3\textwidth}
%\cetnering
%&
%\includegraphics[width=\linewidth / 4]{perf_mean-itemsets-changing-ngram_n1-5_s10+_1hr.png}
%\caption{(B) Mean number of itemsets mined}
%\end{subfigure}%
%\begin{subfigure}{.3\textwidth}
%\cetnering
%&
%\includegraphics[width=\linewidth / 4]{perf_mean-runtime-millis-changing-ngram_n1-5_s10+_1hr.png}
%\caption{(C) Mean runtime in milliseconds}
%\end{subfigure}
%\end{array}$
%\end{center}
%\caption{Effect of the maximum N-Gram length}
%\label{fig:ngramsLen}
%\end{figure}*
%If the number of tokens i
%, and the correct association might be between 
%There is a whole body of work for identifying Multi Word Expressions (MWE) and we have tried some of its measures \cite{belheta}. 

\begin{figure*}
\centering
% Generated with LaTeXDraw 2.0.8
% Sun Apr 28 20:25:46 EDT 2013
% \usepackage[usenames,dvipsnames]{pstricks}
% \usepackage{epsfig}
% \usepackage{pst-grad} % For gradients
% \usepackage{pst-plot} % For axes
% Generated with LaTeXDraw 2.0.8
% Sun Apr 28 20:36:28 EDT 2013
% \usepackage[usenames,dvipsnames]{pstricks}
% \usepackage{epsfig}
% \usepackage{pst-grad} % For gradients
% \usepackage{pst-plot} % For axes
\scalebox{0.45} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-3.56416)(37.82,3.5241601)
\usefont{T1}{ptm}{m}{n}
\rput(5.6938963,-3.2008398){\LARGE (a) Mean number of distinct items}
\usefont{T1}{ptm}{m}{n}
\rput(18.373896,-3.2008398){\LARGE (b) Mean number of itemsets}
\usefont{T1}{ptm}{m}{n}
\rput(32.033897,-3.2208397){\LARGE (c) Mean runtime in milliseconds}
\rput(5.93,0.50416017){\includegraphics{perf_ngramlen1-5_distinct-items_supp10+_1hr.eps}}
\rput(18.89,0.5441601){\includegraphics{perf_ngramlen1-5_itemsets_supp10+_1hr.eps}}
\rput(31.89,0.50416017){\includegraphics{perf_ngramlen1-5_runtime-millis_supp10+_1hr.eps}}
\end{pspicture} 
}
\caption{Effect of the maximum N-Gram length on the mining of 1hr epochs of data}
\label{fig:ngramsLen}
\end{figure*}

After mining term N-Grams we flatten the itemsets to sets of unigrams again. This removes overlap between parts of  itemsets making it easier to reason about how they relate to each other. 
%This also avoids the possibility that an itemset might be represented differently at different epochs. Actually, within one epoch 
This is also necessary since an itemset will have different N-Gram set representations, and its  postings list is the union of those of the different representations.
%, and its support would have been lost if not for maintaining the postings list. 
%We merge the union of the posting lists of all the N-Gram sets flattened to the same itemset.

%After flattening, we remove one more type of uninteresting itemsets produced just because the definition of closed itemsets cover it - the empty itemset and individual unigrams. In the rest of this paper we exclude them altogether, so an itemset  simply by not producing an itemset whose length is less than 2.


%\subsection{Choice of epoch span}
%\label{sec:span}
%Figure \ref{fig:numItemsetsVsSpan} shows the number of itemsets mined from different spans, using the same support threshold. The number of itemsets is higher when the span is shorter than 1 hour because the frequency required to surpass the support threshold  becomes very low (7 on average), and thus more noise itemsets are mined. The flat shape after the 1 hour mark shows that the itemsets mined from the shorter epoch spans are not included in the mining results of longer spans. Therefore, we will mine epochs of 1 hour span for the rest of the paper. 

%The numbers of both closed and maximal itemsets are shown, to give an intuition about the shape of the itemsets prefix tree. The small difference between the closed and maximal itemsets indicates that there is more breadth than than depth in the tree, since there aren't many internal nodes (closed itemsets that are not maximal). We discuss why this structure is formed and how to make use of this intuition in the following section.
%This is a good segway to what we will discuss 

%length of the epoch span used changes the nature of itemsets mined, . 
%\begin{figure}
%\centering
% Generated with LaTeXDraw 2.0.8
% Sat May 04 11:23:51 EDT 2013
% \usepackage[usenames,dvipsnames]{pstricks}
% \usepackage{epsfig}
% \usepackage{pst-grad} % For gradients
% \usepackage{pst-plot} % For axes
%\scalebox{1} % Change this value to rescale the drawing.
%{
%\begin{pspicture}(0,-2.74)(8.22,2.74)
%\rput(4.11,0.0){\includegraphics{mean-itemsets-count_change-with-epochlen_closed-maximal.eps}}
%\end{pspicture} 
%}
%\label{fig:numItemsetsVsSpan}
%\caption{Number of itemsets mined from different epoch spans, using N-Grams as items with N $\le$ 5} 
%\end{figure}


\section{Strongly closed itemsets}
\label{sec:strong}
The closed property of an itemset is very easily violated by modifying one transaction that contains the itemset and removing one of its items. While an update operation is not supported in the model of frequent itemsets mining, a similar effect happens when people are writing about a certain fine grained topic. For example, on November 6th, 2012 many people where tweeting that ``(\underline{Barack} Obama) was elected (\underline{as president \underline{of the United States \underline{of America}}})''. If all the Tweets which use the verb phrase ``was elected'' to report the event also contain the two names (in parenthesis) in full, then there will be one closed itemset with all 12 items. However, either or both the names can be shortened by omitting an underlined part and still convey the meaning. We can consider Tweets with any combination of shortened names to be modifications of the \emph{maximal} closed itemset. Therefore instead of 1 maximal itemset about the event there will be 8 closed ones. Now consider that it is possible to say that ``Obama was elected'' as well as ``Obama got elected'', resulting in 2 maximal itemsets because of a slight variation in the language. Thus, maximal itemsets also has redundancy. We therefore need a condition that is not as easily violated as the closed and maximal conditions for selecting itemsets.

%For example, on November 9th, 2012 many people where tweeting that ``Justin Bieber and Selena Gomez broke up''. If all Tweets which use the verb "break up" to report the topic also contain the two names in full, then there will be one closed itemset with all 6 items. However, a Tweet can contain any of 8 other combinations of the four names that fully convey the meaning. We can consider that Tweets with any of these combinations are modifications of the \emph{maximal} closed itemset. Therefore instead of 1 maximal itemset about the topic there will be 9 closed ones. Now consider that it is possible to say that "Justin and Selena broke up" as well as "Justin broke up with Selena", resulting in 2 maximal itemsets. This increase in the number of maximal itemsets because of slight variations in the language is intensified by the length limit on Tweets, specially when people try to make space for their comment about a Tweet they are retweeting. We therefore need a property between the easily violated closed property and the very strict maximal property which also results in a large number of redundant itemsets. 
% This will result in the creation of two closed itemsets, a new shorter with support that one higher than the original itemset

We define a \emph{high confidence} itemset as a closed itemset whose frequency comprises more than a certain proportion of the frequency of its least frequent subset.
%, or which has at least one strongly closed superset. 
This property chooses closed itemsets which violate the closed property of their subsets by a significant amount. 
%It also filters out itemsets that are short language constructs, because their components appear very frequently and none of their numerous supersets comprise a significant proportion of their high frequency. 
We state this condition formally as follows. Let $\kappa$ be a parameter that can vary between 0 and 1 to increase the selectivity of the condition. Then the set of  \emph{high confidence} itemsets is:
%\begin{equation}\mathcal{S} = \{s:\frac{|T_{s}|}{|T_{s_{parent}}|} \, where \, s_{parent} \subset s \, and \, |T_{s_{parent}}| < |T_{s_{ancestor}} | \forall s_{ancestor} \subset s \}\end{equation}
\begin{equation*}\mathcal{K} = \{s:\frac{|T_{s}|}{|T_{s_{p}}|} \ge \kappa \; where \; s_{p} \subset s \, and \, |T_{s_{p}}| < |T_{s_{a}} |\, \forall \, s_{a} \subset s \}\end{equation*}

Notice that
% within the documents containing a closed itemset, $D_{itemset}$, the probability of a closed superset ($\|D_{superset}\| / \|D_{itemset}\|$) 
$\frac{|T_{s}|}{|T_{s_{p}}|} $ is the called confidence of the rule ``$s_{p} \rightarrow s$.'' This property is the basic property used for association rules mining, and it is used in the definition of $\delta$-free sets. Mining itemsets based on the confidence of rules they induce has long been recognized as a method for finding ``interesting patterns'' \cite{cohen2001finding}, but since this property is not anti-monotone a variation has to be used (for example, \emph{all confidence} \cite{kim2004ccmine}).
%some algorithms for mining itemsets based on a variation called \emph{all confidence} have been proposed \cite{kim2004ccmine}. The strongly closed property builds on the notion of confidence by choosing itemsets which would result in rules of high confidence. The minimum acceptable confidence of the resulting rule is a parameter $\kappa$ that can vary between 0 and 1 to increase the strength of the strongly closed property.

%Using a lower bound on confidence of a longer itemset given a closed subset its least frequent closed subset (maximum confidence of this superset) 
Even though choosing \emph{high confidence} itemsets filters out many redundant itemsets formed because of slight variations of the same topic, it is still does not completely solve the problem of redundancy. The intersection of the sets of transactions containing two \emph{high confidence} itemsets can be different by only 1 transction from either of the sets. For example, the closed itemset \{Obama, Romney\} can have three closed supersets: \{Obama, Romney, debate, \#elections2012\}, \{Obama, Romney, debate\} and \{Obama, Romney, \#elections2012\}. Figure \ref{fig:jelena} illustrates how all supersets can be \emph{high confidence}, while they are still redundant itemsets mined from almost the same Tweets. Each circle in the figure represents the set of Tweets containing the itemset formed by concatenating the words in the circle with the words in its container. The figure also shows, as dashed circles, examples of itemsets that would be filtered out by the \emph{high confidence} property.
\begin{figure}[htb]
\label{fig:jelena}
\centering
% Generated with LaTeXDraw 2.0.8
% Mon Apr 29 15:14:17 EDT 2013
% \usepackage[usenames,dvipsnames]{pstricks}
% \usepackage{epsfig}
% \usepackage{pst-grad} % For gradients
% \usepackage{pst-plot} % For axes
\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-1.26)(4.64,1.26)
\psframe[linewidth=0.018,dimen=outer](4.64,1.26)(0.0,-1.26)
\pscircle[linewidth=0.018,dimen=outer](1.48,-0.12){0.94}
\pscircle[linewidth=0.018,dimen=outer](1.96,-0.12){0.9}
\pscircle[linewidth=0.018,linestyle=dashed,dash=0.16cm 0.16cm,dimen=outer](3.81,0.39){0.27}
\usefont{T1}{ptm}{m}{n}
\rput(1.2158496,1.005){Obama, Romney}
\usefont{T1}{ptm}{m}{n}
\rput(1.1,-0.295){debate}
\usefont{T1}{ptm}{m}{n}
\rput(2,0.1){\small \#elections2012}
\usefont{T1}{ptm}{m}{n}
\rput(3.655244,0.825){governor}
\pscircle[linewidth=0.018,linestyle=dashed,dash=0.16cm 0.16cm,dimen=outer](3.83,-0.47){0.29}
\usefont{T1}{ptm}{m}{n}
\rput(3.6618066,-0.875){president}
\end{pspicture} 
}
\caption{High and low confidence itemsets}
\end{figure}

To remove such redundancy, we merge similar itemsets into \emph{strongly closed} itemset clusters. The \emph{strongly closed} itemset is a bag of items formed by taking the union of all cluster members. 
%This concentrates the numerous itemsets about a certain topic into a few units. 
We do the clustering in the postings list space to avoid merging itemsets 
%including different aspects or opinions about a topic. 
about different topics or different aspects or opinions in the same topic. 
The clustering scheme we use is similar to \cite{bayardo2007scaling}, but it is much simpler. 
For each itemset, we find the itemsets produced before it and overlapping with it, even in one item, such that the \emph{high confidence} condition holds. The itemset joins the cluster containing the itemset with which it achieves the highest confidence. The notion of \emph{confidence} applies for itemsets that are not supersets of one another, but it is defined as: %follows \cite{cohen2001finding}:
\begin{equation}c(s_i \rightarrow s_j) = \frac{|T_{s_i} \cap T_{s_j}|}{|T_{s_i}|}\end{equation}

Thus, we formally define \emph{strong closed} itemset clusters as:
\begin{multline}\label{eq:strongClosedFormal}
\mathcal{S} \gets \, all\,  S \, such\,  that\, 
\\S = \{i:\, i \in \bigcup_{j}{s_j}\; where \; \forall_{s_j}\, \textbf{argmax}_{s_k} c(s_j \rightarrow s_k)  \in S \\\; and\; c(s_j \rightarrow s_k) \le \kappa \} 
\end{multline}
 

%\begin{multline}\label{eq:strongClosedFormal}S = \{i:\, i \in \bigcup{s}\; where \; \forall_{j,k}\, c(s_j \rightarrow s_k)  \le \kappa \} \\
%\mathcal{S} = \{S: s_j \; where \; \textbf{argmax}_{j,k} c(s_j \rightarrow s_k) \; \forall \; s_j \in \mathcal{C}, s_k \in S\}
%\end{multline}

%First, we do an all pairs similarity search on the itemsets, using IDF weighted cosine similarity. 
%Itemsets are clustered  such that \emph{high confidence} is achieved between cluster members and the cluster centroid. We make use of the structure of itemsets for selecting the centroid. The cluster centroid is the most supported itemsets select the The cluster centroid 

%to calculate it between such itemsets the actual (dis-)overlap of the postings lists must be determined. 
%For example, itemsets that differ because of items not pertaining to the topic, such as ``was'' and ``got'' in the Obama-elected example. 
%The use of cosine similarity limits the candidates for merging to a few itemsets that are very likely about the same topic, 
%since the Inverse Document Frequency (IDF) value of terms pertaining to a certain topics is typically much higher than the IDF value of other terms. The IDF and cosine similarity formulae used are given in equations \ref{eq:IDF} and \ref{eq:cosine} respectively.
%\begin{equation}\label{eq:IDF}IDF(i) = \log{\frac{\|E\|}{\|D_i\|}}\end{equation}
%\begin{equation}\label{eq:cosine}cos(S_1,S_2) = \frac{\sqrt{\sum_{i \in S_1 \cap S_2} IDF(i)^2 }}{\sqrt{\sum_{i \in S_1} IDF(i)^2} * \sqrt{\sum_{i \in S_2} IDF(i)^2}} \end{equation}
%To avoid forming alliances that are not coherent about a certain topic, 


Algorithm  \ref{algo:alliance}  shows the implementation for forming \emph{strong closed} itemset clusters.
To see if the \emph{high confidence} condition will be violated, it is enough to check if the difference between the posting lists of two itemsets is above the number given by equation \ref{eq:maxDiffCnt}. In case of merging an itemset with its superset, this difference can be calculated directly from the sizes of the posting lists.
% using the confidence formula.
%by subtracting the support of the superset from that of the itemset. 
Otherwise, the difference can be efficiently calculated from the postings lists since they are sorted, and it is enough to check if the difference exceeds the maximum number allowed. 

% The maximum number of different documents is given by equation \ref{eq:maxDiffCnt}. 
%An empirical performance evaluation is provided in section \ref{sec:perf}. 
%\begin{equation}\label{eq:maxDiffCnt}\Delta(S_1,S_2,\kappa) = (1 - \kappa) * max(\|D_{S_1}\|,\|D_{S_2}\|), \, \kappa \in [0,1[\end{equation}
\begin{equation}\label{eq:maxDiffCnt}\Delta(S_1,S_2,\kappa) = (1 - \kappa) * \min(|T_{s_1}|,|T_{s_2}|)
%, \, \kappa \in [0,1[
\end{equation}


\begin{algorithm}
\SetAlgoLined
\LinesNumbered
\KwIn{$\kappa$: Minimum closed strength} %$\theta$: Minimum cosine similarity, \\
\KwData{F: Frequent Itemsets produced by LCM}
\KwResult{$\mathcal{S}$: Strong closed itemset clusters}
\For{$i \gets 2$ to $|F|$}{
	$K \gets \emptyset$ \tcp*{Candidates for merging with $s_i$}
	$P \gets \emptyset$ \tcp*{Subsets of current itemset}
	\For{$j \gets 1$ to $i - 1$}{
		\If{$|s_i \cap s_j| > 0$}{
			$K$.add($S_j$)\;
			\If{$|s_i \cap s_j| = min(|s_i|, |s_j|)$ and $|s_i| > |s_j|$}{
				$P$.add($S_j$)\;
%				\lIf{$\|S_i\| > \|S_j\|$}{$T$.add($S_j$)}
			}
		}
%		\ElseIf{cos($S_i,S_j$) $\ge \theta$}{
%			$C$.add($S_j$)\;
%		}
	}
	$minDiff \gets \infty$ \tcp*{Best candidate's score}
	\ForEach{$s_k \in K$}{
		\uIf{$s_k \in P$}{
			$\delta \gets |T_{s_k}| - |T_{s_i}|$\;
		}	
		\Else{
			$\delta \gets$ difference($T_{s_i},T_{s_k},\Delta$) \tcp*{Stops early}
		}
		\If{$\delta \le \Delta(s_i, s_k, \kappa)$ and $\delta < minDiff$}{
			$s_m \gets s_k$ \tcp*{Best merge candidate}
			$minDiff \gets \delta$\;
		}
	}
	\If{$minDiff < \infty$}{
		$\mathcal{S}[s_m].itemset \gets \mathcal{S}[s_m].itemset \cup s_i \cup s_m$\;
	}
}
\Return{$\mathcal{S}$}\;
\caption{Forming strong closed itemset clusters}
\label{algo:alliance}
\end{algorithm}



%After joining an alliance an itemset seizes to exist outside of the alliance, so that an itemset can be part of only one alliance. However, when checking which itemsets to consider for merging with a still unallied itemset, its similarity is calculated with the individual itemsets rather than the alliance. Otherwise, an itemset could fail to join an existing alliance because the similarity between the larger bag of items and the itemset is likely to be lower than the similarity between individual member itemsets and the itemset. This can cause cascading many alliances that should have been separate into one large alliance. Such an oversized alliance could also be about different topics. Empirically, we observe that one and only one oversized alliance about different topics is formed. This alliance catches many itemsets made up of low IDF terms, thus not interesting. Topics with high IDF terms in them cannot have high cosine similarity with topics of only low IDF terms. The size of the bag of items in this alliance is significantly larger than other alliances, making it easy to distinguish and discard it.
%It is difficult to reason about how alliances are formed, specially after fi

%Expanding on the previous observation that adding a high IDF term to an itemset with low IDF terms only prevents achieving a high enough cosine similarity, we introduce an optimization that improves both runtime, memory requirements and filtering power. Unlike the original LCM algorithm, our extension requires keeping previous itemsets in memory so that newly generated ones are compared to them to find candidates for alliance. Instead of keeping all previous itemsets in memory we keep only a limited number, $b$. This obviously improves runtime and memory requirements, but it also improves the quality of itemsets chosen for alliance. Given the way PPC-Extension produces itemsets, an alliance candidate appearing more than $b$ itemsets earlier must be similar because of a term that has at least $\log_2(b+1)$  extension items \emph{smaller} than the one that got added to form the current itemset. This lower bound assumes that all possible itemsets created by appending any combination of the $\log_2(b+1)$ will be closed and will have enough support. In this case, there will be 1 combination where all the items are appended, $\log_2(b+1)$ combinations where one item is missing, $\log_2(b+1)$ choose 2 combinations where 2 items are missing, ... etc. For all of these combinations to be closed itemsets, the 1 combination where all the items are appended has to have minimum support, the  $\log_2(b+1)$ combinations where 1 item is missing will each have support 1 more than minimum, the combinations where 2 items are missing will each have support 2 more than minimum, ... etc. Therefore the minimum drop in frequency between a closed itemsets and its subset produced $b$ steps earlier is given by:
%\begin{equation}\|D_{S_x}\| - \|D_{S_(x-b)}\| = \alpha \frac{\|E\|}{E.span} + \sum_{a=1}^{\log_2(b+1) - 1}a\binom{\log_2(b+1)}{a} \end{equation}
%To minimize the difference in frequency between the current itemset and the alliance candidate, we assume that the 1 combination will appear $\ciel{b/2}$ times,  Therefore the difference in frequency between  $b$ 

%A similar situation also happens if the intersection ???
%must have a frequency of occurrence at least $b$ more than the current itemset's frequency. 

%Algorithm \ref{algo:alliance} shows the described algorithm for merging itemsets alliances. Table  \ref{table:allianceQuality} shows the number of closed itemsets of length at least 2 without filtering any of them out, as well as after applying the KLD filter and the strongly closed filter, and the number of itemsets alliances. The table also shows the average quality of the itemsets after each stage of reduction. The quality is calculated as Basic Elements? PERPLEXITY? CASCADE MEASURE? 

\subsection{Bounding runtime and memory}
\label{}
%Expanding on the previous observation that adding a high IDF term to an itemset with low IDF terms only prevents achieving a high enough cosine similarity, 
We exploit the way PPC-Extension produces itemset to introduce an optimization that improves both runtime, memory requirements and filtering power of the high confidence condition. Unlike the original LCM algorithm, filtering low confidence itemsets requires keeping mining resluts in memory to calculate the confidence of newly generated ones.
% forming strong closed clusters requires keeping itemsets in memory so that newly generated ones are clustered with them.
% to find candidates for merging. 
Instead of keeping all previous itemsets in memory we keep only a limited number, $b$ (for buffer size). This obviously improves runtime and memory requirements, and it can also improve the choice of itemsets. If the total ordering of items follows the descending order of items' frequencies, then the call tree of recursive LCM calls will be very similar to the FP-tree. Given the length constraint on Tweets, then the FP-Tree can only be shallow and wide. 
%For a large enough $b$, 
%if an itemset is produced $b$ steps ago then this itemset's overlap with the current itemset is very likely a language construct. Since the tree is shallow, then 
Therefore, an itemset that has a large number of supersets cannot be appearing with only a limited set of words (topic), but it has to be covering a breadth of possible descendants. Thus this itemset is very likely to be a language construct. Even if it is a language construct that appears with high confidence with some topical words, this high confidence doesn't carry interesting information; for example, \{the, of\} that appears within \{the, united, states, of, america\}. Even if it is a topical word with very high frequency, then interesting information will be divided into several topics given its large number of direct descendants. In either case, filtering closed itemsets that have high confidence with such a ubiquitous itemset will not omit any information. If the filtered itemset is part of an informative one then it will be produced when the longer itemset is found to have high confidence with the filtered one.

We use a buffer size $b=1000$ in all experiments, which leads to a runtime of about 1 to 3 milliseconds per itemset for filtering out low confidence itemsets and creating strong closed clusters. The number of selected itemsets is also reduced by almost half on average. The difference between the two selections is made up mostly of stop words. A few itemsets including topical words are also filtered, such as the closed itemset \{president, Obama\} since Obama is an example of a ubiquitous topical word. Such itemsets lack any information on their own and is included in other informative ones. Finally, this enhancement is not necessary for satisfactory performance. With an unlimited buffer, the runtime is XXX. 
%Furthermore, the clustering is improved for some itemsets because of filtering noisy ones. For example, ``ballot''
%only be a very frequent set of items. The number
%, because of the way PPC-Extension produces itemsets, for an itemset $S_x$ and any candidate subset $S_{(x-b)}$ that was produced $b$ itemsets earlier $\|D_{S_{(x-b)}}\| - \|D_{S_x}\| \ge b$. This lower bound is achieved when the intersection of the documents containing the current itemset $S_x$ and all the $b-1$ supersets before it is exactly the subset $S_{(x-b)}$, and there are no more itemsets with the same intersection. In this case each of the $b$ supersets must have support at most $\|D_{S_{(x-b)}}\| - 1$ to be considered closed, but because there are $b$ of them then actually their support is $\|D_{S_{(x-b)}}\| - b$. Since the minimum support difference is $b$, then the maximum confidence of $S_x$ is $\frac{\|D_{S_{(x-b)}}\| - b}{\|D_{S_{(x-b)}}\|} $. Therefore for a given $\kappa$ we can determine the buffer size $b$ as the frequency of itemsets keeps decreasing. In our implementation we use a fixed $b$ of 1000.


% The difference between documents containing a certain strongly closed itemset and the intersection can be as little as one document. Thus all variations can have high confidence. On the other hand, using an upper bound on confidence is totally counter intuitive, since this means that if only one document exists with a shorter itemset then the longer itemset will be omitted. 
%Observe that all documents containing the longer itemset will also contain the shorter itemset, and it is possible for several variations to have high confidence because documents
%filters out many redundant closed itemsets, but cannot filter out formed because of slight variations of the same topic. 

%We build on the confidence property by merging high confidence itemsets that are extensions of the same itemset into an itemset \emph{alliance}. The itemset alliance is a bag of items formed by taking the union of all member itemsets. This concentrates the numerous high confidence closed and maximal itemsets about a certain topic into one unit. Furthermore, we also include in the alliance all itemsets with high cosine similarity to its members. This merges together itemsets that differ because of items that come earlier in the total ordering and are not pertaining to the topic, such as ``with'' and ``and'' in the Bieber-Gomez example. To avoid forming alliances that are not coherent about a certain topic, we go beyond the numerical calculation of confidence and we actually calculate the overlap between the posting lists of itemsets being merged. In case of merging an itemset with its subset 
%With a reasonably high support, the 
\subsection{Effect of confidence threshold}%Choice of $\kappa$}
Figure \ref{fig:numStrongHConf} shows the number of high confidence itemsets (of length more than 1) at different values of $\kappa$. Even at low values of $\kappa$, the number is much lower than the number of closed itemsets (6147 from figure \ref{fig:ngramsLen}, but only 2440 after removing 1-itemsets). This encourages us to use a low value of $\kappa$, since the biggest reduction in number of itemset has already happened by filtering out itemsets that don't make up any significant proportion of their subsets'  support. We use $\kappa=0.2$ for the rest of the paper. 

The number of strong closed itemset clusters, and its ratio to the number of high confidence itemsets are also shown. While the number of high confidence itemsets decrease as $\alpha$ increases, the ratio stays almost fixed. This indicates that regardless of the selectivity of the high confidence condition (or any other condition used) there will always be itemsets that can be clustered together, stressing the effect of language use. At $\kappa=0.2$, we are left with ~10\% of the closed itemsets of length 2 or more. The next two sections will show how to make sense of them.
%the selectivity of strong closed itemset clusters is $~\frac{200}/{2440}

\begin{figure}
% Generated with LaTeXDraw 2.0.8
% Sun May 05 22:12:01 EDT 2013
% \usepackage[usenames,dvipsnames]{pstricks}
% \usepackage{epsfig}
% \usepackage{pst-grad} % For gradients
% \usepackage{pst-plot} % For axes
\scalebox{0.5} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-5.37)(16.96,5.37)
\rput(8.48,0.0){\includegraphics{num-itemsets_high-conf+strong-closed_against-kappa.eps}}
\end{pspicture} 
}


\label{fig:numStrongHConf}
\caption{Number of strong closed itemsets and high confidence itemsets at different values of $\kappa$}
%. The ratio of the two numbers is shown on top of the bar}
\end{figure}

\section{Temporal Ranking}
\label{sec:allianceRank}
So far we have been reducing the number of itemsets, and we succeeded to reduce it to about 1\% of the original number (in alliances). We now discuss how to rank the itemset alliances, making use of the structure of the alliance. 
%We have experimented with various ranking schemes applied directly to itemsets (before and after reduction) and none of them resulted in a satisfactory ranking. 
Again, we exploit the pointwise KL Divergence from a background model. The pointwise KLD of the probability of an itemset $S_i$ in the background model $Q$ from its probability in the current epoch $P$ can be considered the information gain: 
\begin{multline}IG(P(S_i),Q(S_i)) = KLD(P(S_i)||Q(S_i)) \\ = - (H(P(S_i)) - H(P(S_i),Q(S_i))) \\ = \sum{P(S_i) \log{P(S_i)}} - \sum{P(S_i) \log{Q(S_i)}} \end{multline}

We note that the joint probability of the itemset and its superset is equal to the probability of the superset; $P(S_i,S_j) = P(S_j)$. Thus, the information gain of the appearance of an itemset and its superset is essentially the information gain of the superset.  Also, their Pointwise Mutual Information is the Self Information of the subset:
\begin{multline}
PMI(S_i, S_j) = \log{ \frac{P(S_i,S_j)}{P(S_i)P(S_j)} } = -\log{P(S_i)} = I(S_i)
\end{multline}

Therefore, the information gain of a superset is different from the information gain of its subset by the information gained (or lost) because of the additional items. Thus, the information gain of the appearance of all itemsets in an alliance of $m$ itemets, $S_{i1},S_{i2},...S_{im}$, can be calculated as the information gain of its smallest subset, $S_{min}$, plus the differences between the KLDs of member subsets and the smallest subset. We use the squares of the differences, because the pointwise KLD value might be positive or negative, depending on whether the subset appears with lower or higher probability than in the background. Thus the information gain of an alliance is given by:
\begin{multline*}IG(P(S_{i1},...S_{im}),Q(S_{i1},...S_{im})) = I(S_{min}) + \\ %S_{i2},
\sum_{j = i1..im}{(KLD(P(S_j)||Q(S_j)) - KLD(P(S_{min})||Q(S_{min})))^{2}} \end{multline*}
%the squares of
%The difference is squared because the difference in information gain can be negative, however the information content is positive:
%can be explained in terms of mutual information. It is the mutual information between the itemset's appearance in  and 
%structure of the alliance produced by PPC-Extension if the total ordering follows the descending order of items' frequencies.

Finally, the average information gain of an itemset in an alliance is give by:
\begin{equation}\label{eq:avgIG}\overline{IG}_{alliance} = \frac{IG(P(S_{i1},...S_{im}),Q(S_{i1},...S_{im}))}{m}\end{equation}

The ranking of itemset alliances according to equation \ref{eq:avgIG} gives superior results to many other ranking schemes we tried, including ones based on scores derived from the itemset content directly. An intuitive explanation is that this ranking scheme gives high scores to itemsets alliances made up of a minimum set with small KLD and supersets with large KLD. According to the structure of the alliance produced by PPC-Extension if the total ordering follows the descending order of items' frequencies, the small KLD minimum subset will be an entity with sustained interest, and the high KLD supersets will be new appearing with probabilities very different from their background probabilities. In the next section we show the results of applying this ranking scheme along with the methods described in previous sections.

\section{Empirical Evaluation}
\label{sec:empirical}
We have shown in the last 3 sections how to improve the efficiency of Frequent Itemsets Mining on social media data, by reducing the number of itemsets produced. To show the performance of the proposed methods in 
%filtering out itemsets that are uninteresting and 
choosing important itemsets, we apply them on Tweets posted on the $6^{th}$ and $9^{th}$ of November 2012. In tables \ref{table:nov6} and \ref{table:nov9} we show the results of mining overlapping 1 hour epochs. Because of space limitation we show only the top 3 itemsets, ranked as described in section \ref{sec:allianceRank}, for each hour. Times are in the EST zone.
%by point wise KL-Divergence against the results of mining the month of October. 

On November $6^{th}$, the US presidential elections took place and we can see how its events unwind from ``get out and vote'' to the projection of ``obama to win'' and the votes counts (only ``163, 172'' made it to the top 3 at 20:30), all the way to the ``acceptance speech'' and the social media attention given to the ``lady behind obama  with a flag in her hair''. Early in the day, the ``goal [de] Pepe'' scored for ``Real Madrid'' and other itemsets about UEFA Champions football games appear in the top 3. At 22:30 the news that ``weed is legal in Colorado'' breaks into the top 3 because of its novelty, even though this is short after Obama's victory was declared. Throughout the day itemsets about the elections as well as about other topics are ranked high, even if they are topics in a language other than English and thus with a smaller supporting user base (such as the topic in Portuguese about when to consider a person old [velho]).
%UEFA Champions football games! We can 

On November $9^{th}$, no major events were happening but many overlapping minor ones happened. The day starts by news about the end of two careers; the Laker's ``coach Mike Brown'' got fired  and ``CIA director David Petraeus resigns''. The relationship of Justin Bieber also ends as he ``broke, up [with], Selena'' at 22:00 (the name of  Bieber is included in other itemsets  not in the top 3). The MTV Europe Music Awards (MTVEMA) was also taking place and votes were solicited from audience through Twitter. This is an example of a topic where people have different opinions, as the hour 15:00 shows. The top 3 itemsets of this hour are supporting ``Katy  Perry'', ``Lady Gaga'' and ``Justin Bieber'' respectively, and they are all reported as separate itemsets. The neutral itemset only soliciting votes is also mined and appears in the top 3 of other hours. By the end of the day many congratulations for the Indonesian Hero's day (``Hari Pahlawan'') appear, and the Turkish commemoration day of Ataturk is also mentioned as the 10th starts in these countries. 

% (we refer to it as the \emph{MTVEMA voting invitation} to save space).
%To start off, the Laker's ``coach Mike Brown'' got fired and it was expected that either ``Phil Jackson [or] Jerry Sloan'' will succeed him. Another career end gets attention as ``CIA director David Petraeus resigns'', and an The MTV Europe Music Awards (MTVEMA) was also taking place and he also broke up with Selena Gomez. Therefore, this period contains events of different types that overlap, and we can use it to assess the effectiveness of our methods without access to ground truth data. The itemset alliances shown are the top 10 per hour ranked by point wise KL-Divergence against the results of mining the month of October. 
\begin{table}

\begin{center}
\small
\begin{tabular}{|p{.6cm}|p{2.5cm}|p{5cm}|}

\hline
\multirow{3}{*}{\texttt{13:00}} 	&  0, 1, de, jong 			& De Jong scores for Ajax \\ \cline{2-3} %http://www.dailymail.co.uk/sport/football/article-2228815/Manchester-City-2-Ajax-2--match-report-Siem-Jong-Yaya-Toure-Sergio-Aguero-Champions-League.html
					   	& geordie, shore		& Season 5 of the TV series starts \\ \cline{2-3}
						& get, out, and, vote		& Still early in the U.S. elections day \\\hline

\multirow{3}{*}{\texttt{17:00}} 	&  if, obama, wins		& Speculations regarding elections \\ \cline{2-3}
					   	& @noemiking20, club, my, spots		&  Pyramidal marketing scam. Retweeted by people to make money.  \\ \cline{2-3}
						& the, polls, close		& Polls to start closing at 6 PM \\\hline %http://www.huffingtonpost.com/2012/11/06/what-time-do-polls-close_n_2080894.html

\multirow{3}{*}{\texttt{19:00}} 	&  A partir de que idade voc\^{e}a considera algu\'{e}m velho?		& Internet meme from Brazil, discussing when to start considering a person old. \\ \cline{2-3}
					   	& food, stamps		& Discussions pertaining to elections \\ \cline{2-3}
						& linda, mcmahon, senate		&  Linda McMahon loses CT senate race \\\hline

\multirow{3}{*}{\texttt{20:00}} 	& obama, got, this		&  Announcing states that Obama got \\ \cline{2-3}
					   	& projected, winner		& Early projections about who will win \\ \cline{2-3}
						& moving, to, canada		&  Reaction to projections \\\hline


\multirow{3}{*}{\texttt{21:00}} 	& elizabeth, warren		&  MA senate elections winner \\ \cline{2-3}
					   	& popular, vote		& vs Electoral Vote \\ \cline{2-3}
						& who, is, the, president?		&  Who? Who? Who? \\\hline
						
						
\end{tabular}
\end{center}
\caption{Top 3 itemsets for hours in November 6th}
 \label{table:nov6}
\end{table}


\begin{table}
\begin{center}
\small
\begin{tabular}{|p{.6cm}|p{7.5cm}|}
\hline %\textbf{Top 3 itemsets for hours in November 6th,  2012} \\\hline
\multirow{3}{*}{\texttt{12:00}} & and, get, out, vote \\ & @laliminati, lali \\ & geordie, shore \\\hline 
\texttt{12:30} & 0, 1, de, jong - geordie, shore - if, obama, wins \\\hline 
\texttt{13:00} & de, gol, pepe - hala, madrid - geordie, shore \\\hline 
\texttt{13:30} & de, gol, pepe - geordie, shore - for, i, voted \\\hline 
\texttt{14:00} & for, i, voted - for, obama, vote - fuera, juego \\\hline 
\texttt{14:30} & and, basketball, love - for, i, voted - el, madrid, real \\\hline 
\texttt{15:00} & \#countkun, 11, 6 - for, i, voted - eu, te, vivo \\\hline 
\texttt{15:30} & \#geordieshore, @charlottegshore - @sophiaabrahao, URL, live, on - academy, tool \\\hline 
\texttt{16:00} & if, romney, wins - geordie, shore - if, obama, wins \\\hline 
\texttt{16:30} & geordie,shore - if,obama,wins - @noemiking20,club,spots \\\hline 
\texttt{17:00} & a, algu\'{e}m, considera, de, idade, partir, que, velho, voc\^{e}  - for, i, voted - if, obama, wins \\\hline 
\texttt{17:30} & virginia, west - election, is, the, this - food, stamps \\\hline 
\texttt{18:00} & a, algu\'{e}m, considera, de, idade, partir, que, velho, voc\^{e}  - election, is, this - virginia, west \\\hline 
\texttt{18:30} & food, stamps - a, algu\'{e}m, considera, de, idade, partir, que,  velho, voc\^{e}  - linda, mcmahon, senate \\\hline 
\texttt{19:00} & \#stayinline, in, line - got, obama, this - a, algu\'{e}m, considera, de, idade, partir, que, velho, voc\^{e} \\\hline 
\texttt{19:30} & got, obama, this - projected, winner - canada, move, moving, to \\\hline 
\texttt{20:00} & obama,to,win - election,the,watching - projected, winner \\\hline
\texttt{20:30} & elizabeth, warren - popular, vote - 163, 172 \\\hline 
\texttt{21:00} & \#forward, \#obama2012 - election, is, this - is, my, president, still \\\hline 
\texttt{21:30} & \#forward,\#obama2012 - of,president,the - back,in,office \\\hline 
\texttt{22:00} & \#forward, \#obama2012 - colorado, in, legalized - black, go, never, once, you \\\hline 
\texttt{22:30} & colorado,in,legal - food,stamps - colorado,in,is,legal,weed \\\hline 
\texttt{23:00} & acceptance, speech, wrote - in, is, legal, weed - cnn, on \\\hline 
\texttt{23:30} & come, to, yet - est, mais - colorado, move, to \\\hline 
\texttt{00:00} & come, to, yet - behind, flag, hair, her, in, obama - flag, that, weave \\\hline
 \end{tabular}
 \caption{Top 3 itemsets for hours in November 6th}
 \label{table:nov6Old}
 \end{center}
 \end{table}
 
 
 
  
 
 \begin{table}
\centering
\small
\begin{tabular}{|p{.6cm}|p{7.5cm}|}
\hline %\textbf{Top 3 itemsets for hours in November 6th,  2012} \\\hline
\texttt{12:00} &   \#iwillneverunderstand, why -   brian, shaw -   breaking, brown, coach, head, mike \\\hline
\texttt{12:30} &   @boyquotations, do, everyone, follow, followers, gain, more, rt, s, want, who, you \emph{(written as `@boyquotations,..' later)}  -   brian, shaw -   jackson, jerry, phil, sloan \\\hline

\texttt{13:00} &    `@boyquotations,..' -   ao, lado, sempre, seu -   brian, shaw \\\hline

\texttt{13:30} &   09, 11 -   \#tvoh, babette -   \#tvoh, ivar \\\hline

\texttt{14:00} &   futuro,ser\'{a} -   acha,que,voc\^{e} -   cia,david, director,petraeus, resigns \\\hline

\texttt{14:30} &   \#emawinbieber, \#mtvema, URL, at, be, bieber, big, i, justin, pick, the, think, tweet, will, winner, your \emph{(written as `\#emawinbieber,..')} -   give, love, me -   \#tvoh, johannes \\\hline

\texttt{15:00} &   \#emawinkaty, \#mtvema, URL, at, be, big, i, katy, perry, pick, the, think, tweet, will, winner, your -   \#emawingaga, \#mtvema, at, big, pick, the, tweet, winner, your -   `\#emawinbieber,..' \\\hline

\texttt{15:30} &   `\#emawinbieber,..' -   \#atat\"{u}rkenot, atam, atat\"{u}rkenot -   qui, veut \\\hline

\texttt{16:00} &   brown, mike -   `\#emawinbieber,..' -   qui, veut \\\hline

\texttt{16:30} &   brown, mike -   \#qvemf, lilou -  `\#emawinbieber,..' \\\hline

\texttt{17:00} &   brown, mike -   bulan, lahir -   @venomextreme, venom \\\hline

\texttt{17:30} &   `@boyquotations,..' -  `\#emawinbieber,..' \\\hline

\texttt{18:00} &   \#ullychat, qual -   \#ullychat, @ullylages, qual -   brown, mike \\\hline

\texttt{18:30} &   o, pensado, que, sobre, tem, ultimamente, voc\^{e} -   URL, business, i, online -   coffee, green \\\hline

\texttt{19:00}&   `@boyquotations,..' -   o, que, tem, voc\^{e} -   o, pensado, que, sobre, tem, ultimamente, voc\^{e} \\\hline

\texttt{19:30}&   \#iwillneverunderstand, why -   o, pensado, que, sobre, tem, ultimamente, voc\^{e} -   \#mtvema, URL, at, be, big, pick, the, tweet, will, winner, your \\\hline

\texttt{20:00} &   \#iwillneverunderstand,why -   o, pensado, que, sobre, tem, ultimamente, voc\^{e} -   o,pensado,que,tem,ultimamente,voc\^{e} \\\hline

\texttt{20:30} &   hari, pahlawan, selamat -   \#mtvema, URL, at, be, big, pick, the, tweet, will, winner, your -   \#mtvema, URL, at, be, big, the, tweet, will, winner \\\hline

\texttt{21:00} &   hari, pahlawan, selamat -   \#mtvema, URL, at, be, big, pick, the, tweet, will, winner, your -   \#iwillneverunderstand, why \\\hline

\texttt{21:30} &   hari, pahlawan, selamat -   \#iwillneverunderstand, why -   pick, your \\\hline

\texttt{22:00} &   hari, pahlawan, selamat -   and, broke, selena, up -   \#iwillneverunderstand, why \\\hline

\texttt{22:30}&   \#asdtanya, \#hmmomspik, @ahspeakdoang -   \#iwillneverunderstand, why -   hari, pahlawan, selamat \\\hline

\texttt{23:00} &   \#iwillneverunderstand, why -   brown, mike -   hari, pahlawan, selamat \\\hline

\texttt{23:30}&   10, nov -   brown, mike -   10, 11, 2012 \\\hline

\texttt{00:00} &  ataturk, aniyoruz, kemal, mustafa -   hari, pahlawan, selamat -   \#iwillneverunderstand, why \\\hline
 \end{tabular}
 \caption{Top 3 itemsets for hours in November 9th}
 \label{table:nov9}
 \end{table}

 
 
 
 %To validate that filtered itemsets don't contain important information, we list all items that appear in filtered out itemsets but not in any  chosen itemset. If an itemset is not fully contained in any of the chosen ones, then the minimum difference it had with any of the chosen ones can possibly contain information not present in the selection. Table \ref{table:filteringPerf} shows the differences between  itemsets containing any of the terms \texttt{barack, obama, mitt, romney, justin, bieber, selena, gomez}, and the selection at different values of $\kappa$. We do not show all the filtered items because of space constraints, but we pick the most ..... If a number appears beside an item, it is the number of epochs when this item appeared and didn't get selected.
%For each itemset in the full collection we find the minimum difference between it and itemsets selected by the selection criterion we proposed.


\section{Related work}
\label{sec:related}

\section{Conclusion and future work}
\label{sec:concfut}

\section{Acknowledgement}

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{younos_cikm2013}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns

\end{document}
